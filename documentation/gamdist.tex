\documentclass[12pt]{article}
\usepackage{fullpage,graphicx,psfrag,amsmath,amsfonts,verbatim}
\usepackage[small,bf]{caption}

\input defs.tex

\bibliographystyle{alpha}

\title{gamdist: Generalized Additive Models in Python}
\author{Bob Wilson}

\begin{document}
\maketitle

\begin{abstract}
TBD
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}
This paper introduces a Python library called \texttt{gamdist}, which uses a distributed optimization technique called the Alternating Direction Method of Multipliers (ADMM) to fit a special type of regression model called a Generalized Additve Model (GAM)  to data. 

\paragraph{Outline of Paper} In \S\ref{sec:gam} we describe Generalized Additive Models. In \S\ref{sec:admm} we describe the Alternating Direction Method of Multipliers and how it may be used to fit GAMs. In \S\ref{sec:arch}, we describe the architecture of the library, including relevant implementation details.

\section{Generalized Additive Models}
\label{sec:gam}

The primary goal of \texttt{gamdist} is the estimation of certain aspects of the joint distribution of a collection of one or more random variables $X$ called \textit{features} and a random variable $Y$ we will call the \textit{response}. Specifically, we are interested in the conditional distribution of $Y \given X$. Perhaps the simplest approach is the linear model, which assumes $Y \given X = x \sim \normal (\mu(x), \sigma^2)$, where $\mu(x) = \nu(x)^T \beta$. This notation captures three key assumptions of the linear model. First, the conditional distribution is Gaussian for all values of $X$. Second, the mean of the distribution depends on the features $X$ in a fairly specific way discussed below. Third, the variance is the same for all values of $X$. These assumptions are all loosened in various generalizations of the linear model used in \texttt{gamdist}.

If we choose $\nu(x) = x$, then the assumption is that $\mu(x) = x^T \beta$, and the mean depends linearly on the features; however, the \textit{linear} in \textit{linear model} refers to the dependence on $\beta$, not on the features. It is common to include a constant term in $\nu(x)$ to account for an affine dependency between the features and the response. For example, $\nu(x) = \begin{bmatrix} 1 & x_1 & x_2 & \cdots \end{bmatrix}^T$. We might incorporate quadratic terms to capture nonlinear dependencies including interactions, such as $\nu(x) = \begin{bmatrix}x_1 & x_2 & x_1^2 & x_1 \cdot x_2 & x_2^2 & \cdots \end{bmatrix}^T$. Or we might include more exotic transformations of the features, like $\nu(x) = \begin{bmatrix} \log(x_1) & \sin(x_2) & \cdots \end{bmatrix}^T$. These models are nonlinear in the features, but linear in the parameters $\beta$; however, the linear model will only incorporate transformations that we explicitly include.

Another common situation is when some or all of the features are categorical. For example, consider a model with a single feature corresponding to a person's favorite color, and suppose choices are limited to red, green, and blue. The model would consist of the average responses for people who prefer any particular color. We might support such a model by defining
\begin{displaymath}
   \nu(x) = \begin{cases}
       \begin{bmatrix} 1 & 0 & 0 \end{bmatrix}^T & \textrm{if $x = $ red} \\
       \begin{bmatrix} 0 & 1 & 0 \end{bmatrix}^T & \textrm{if $x = $ green} \\
       \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}^T & \textrm{if $x = $ blue.}
   \end{cases}
\end{displaymath}
Alternative approaches to encoding categorical variables are common and useful in different circumstances. We see that the simple linear model is applicable to a wide range of problems, even those that may not appear linear at first glance.

Fitting a linear model to a set of observations is called linear regression, and is accomplished by solving a least squares optimization problem. This is an example of maximum likelihood estimation (MLE), itself a special case of maximum a posteriori (MAP) estimation, which is the unifying approach used throughout \texttt{gamdist}. It is worth formulating this optimization problem so that we may see how it evolves as we consider more general scenarios.

Suppose we have $n$ observations of the form $\{ (x^{(i)}, \,y^{(i)} ) \}_{i=1, \ldots, n}$. We assume these observations are independent and drawn from the same (unknown) joint distribution.\footnote{This assumption is loosened in random effects or mixed effects models which are not considered here; see \cite{Stroup:2012}.} Under the assumptions of the linear model, $Y \given X = x^{(i)} \sim \normal (\mu(x^{(i)}), \sigma^2)$. The likelihood of a particular observation is
\begin{displaymath}
    \mathcal{L}(\beta; x^{(i)}, y^{(i)}) = \frac{1}{\sqrt{2\pi \sigma^2}} \cdot \exp\left( -\frac{1}{2 \sigma^2} \left(y^{(i)} - \nu \left( x^{(i)} \right)^T \beta \right)^2 \right).
\end{displaymath}
Note that by convention the likelihood is interpreted as a function of $\beta$ parameterized by the observation $(x^{(i)}, y^{(i)})$. The likelihood of the entire set of observations is the product of the likelihoods of the individual observations: $\mathcal{L}(\beta; x, y) = \Pi_{i=1}^n \mathcal{L}(\beta; x^{(i)}, y^{(i)})$. The log-likelihood is the sum of the log-likelihoods of the individual observations:
\begin{displaymath}
   \ell(\beta; x, y) = \log \mathcal{L}(\beta; x, y) = -n/2 \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n \left( y^{(i)} - \nu \left(x^{(i)} \right)^T \beta \right)^2.
\end{displaymath}
Maximizing the likelihood is the same as maximizing the log-likelihood, and if we are only interested in estimating $\beta$, this is equivalent to the problem
\begin{displaymath}
        \begin{array}{ll}
            \mbox{minimize} & {\displaystyle \sum_{i=1}^n} \left( y^{(i)} - \nu \left(x^{(i)} \right)^T \beta \right)^2,
        \end{array}
\end{displaymath}
where the variable is $\beta$ and $y^{(i)}$ and $\nu(x^{(i)})$ are data. An elementary result in optimization theory is that a unique solution exists if and only if $V^T V$ is full rank, where the $i$th row of $V$ is equal to $\nu(x^{(i)})^T$. In that case, the optimal $\beta$ satisfies the so-called normal equations:
\begin{displaymath}
    V^T V \hat{\beta} = V^T  y.
\end{displaymath}

If we assume the model is correct (that is, that the conditional distribution really has the assumed form), exact formulae exist for confidence intervals on the parameters $\beta$. If the variance is unknown, it too can be estimated from the data. We can employ hypothesis tests against the null hypothesis that some or all of the components of $\beta$ are zero. We can apply the resulting model to new data assumed to be drawn from the same joint distribution to compute confidence intervals on the response, $Y \given X = x_\textrm{new}$ or the mean of this distribution, $\mu(x_\textrm{new})$. These are immensely valuable tools in the analysis of data and the application of data to predictions. A good reference on linear models is \cite{Weisberg:2005}. Useful results are collected in Appendix~\ref{app:linear_model}.

We can loosen the assumption of constant variance slightly without materially affecting the inferences that may be drawn. Suppose the $i$th observation has variance $\sigma^2 / \phi^{(i)}$, where $\phi^{(i)}$ is a known quantity, but $\sigma^2$ may or may not be known a priori.

Even if the assumptions underlying the linear model are correct, if the noise is high, or if the number of features is large relative to the number of observations, we may use regularization to improve both the estimates of $\beta$ and predictions based on the estimated model. Regularization reduces the sensitivity of the estimates to noise at the expense of introducing bias, and may be thought of as imposing a Bayesian prior on the parameters $\beta$. Some of the most common forms of regularization include ridge regression and the lasso \cite{Tibs:96}. For example, the lasso may be formulated as the problem:
\begin{displaymath}
        \begin{array}{ll}
            \mbox{minimize} & {\displaystyle \sum_{i=1}^n} \left( y^{(i)} - \nu \left(x^{(i)} \right)^T \beta \right)^2 + \lambda \cdot \| \beta \|_1,
        \end{array}
\end{displaymath}
but this problem does not have a closed-form solution. Moreover, introducing regularization means the formulation is no longer a maximum likelihood estimation problem. Instead, it is a maximum a priori estimation problem. MLE problems have some statistical properties that MAP estimation problems do not possess.

If the constant variance assumption does not hold, but the dependence is known, then Weighted Least Squares.

If the conditional distribution is not Gaussian, other techniques may prove more useful. For example, if the conditional distribution is Laplacian, we may use least absolute deviation regression instead of least squares \cite{Birkes:1993}. Like with the lasso, there are no exact formulae for statistical inference in this context and we must settle for an asymptotic or non-parametric approach such as the bootstrap \cite{Efron:1993}.

Yet another approach to extending linear models was introduced by \cite{NW:72} and discussed in detail in \cite{MN:1983}. Their formulation extends the linear model in a few ways. The conditional distribution is not assumed to be Gaussian. Common alternatives include the binomial and Poisson distributions. The mean of the distribution is permitted to depend on the features in a more complicated way, via the introduction of a \textit{link function}, $g$: $\mu(x) = g^{-1}(\eta(x))$, where $\eta(x) = \nu(x)^T \beta$. When $g(x) = x$, this recovers the same relationship between the features and $\mu$ assumed in the linear model, but other link functions may be used like the logistic function $g(x) = \log(x / (1-x))$. Finally, the variance is sometimes permitted to depend on $x$ instead of being constant. Such models are called Generalized Linear Models (GLMs). Fitting such models is accomplished via maximum likelihood estimation:
\begin{displaymath}
        \begin{array}{ll}
            \mbox{minimize} & {\displaystyle \sum_{i=1}^n} \ell\left( \beta; x^{(i)}, y^{(i)}\right) + r(\beta),
        \end{array}
\end{displaymath}
where $r(\beta)$ is a regularization term on $\beta$, such as in the lasso. For many choices of distribution family and link function, the corresponding optimization problem is convex.

Introduced by \cite{HT:86}, Generalized Additive Models (GAMs) extend GLMs by permitting $\eta(x)$ to be a nonparametric function of the features: $\eta(x) = \sum_{i=1}^p h_i (x_i)$, where $h_i$ are smooth functions. When $h_i(x_i) = \beta_i x_i$, the linear model is recovered (all of the parametric dependencies discussed with regards to $\nu$  are still possible here of course, but the idea is that the data itself should tell us the form of the relationship). Typically the $h_i$ functions are chosen to be some sort of spline, such as a natural cubic spline. GAMs are also fit via MLE, and many practical problems can be formulated as convex optimization problems.

(Reference GAM, GAMr, Casella and Berger, Seber) regularized GAMs 

\section{The Alternating Direction Method of Multipliers}
\label{sec:admm}

\section{Software Architecture}
\label{sec:arch}

\section*{Acknowledgments}

This is an example of an unnumbered section.

\appendix

\section{Properties of the Linear Model}
\label{app:linear_model}

In this section, we state various useful facts (without proof) regarding the linear model. For details, see \cite{Weisberg:2005}, \cite{Seber:2003}, \cite{Wood:2017}, and \cite{CB:2001}. The goal of this section is twofold: to gather formulae used in \texttt{gamdist}, and to highlight what we typically want to do in a regression analysis, beyond just fitting the model. In fact, it is often desirable to quantify the uncertainty in fitted model parameters, check the assumptions of the model through examination of the residuals, perform model selection, and quantify the uncertainty associated with predictions.

The beauty of the linear model is that exact formulae are attainable in support of these goals. Other types of regression only support asymptotic or approximate formulae. However, it is rarely the case that the assumptions of the linear model are expected to hold exactly. If the assumptions are approximately valid, we may hope the formulae which follow are approximately valid. Much has been written about the robustness of these formulae under deviations from the assumptions \cite[\S 9]{Seber:2003}. We are inspired by the maxim \cite{Box:1987}, ``All models are wrong, but some are useful."

Suppose $Y \given X = x \sim \normal (\mu(x), \sigma^2)$, where $\mu(x) = \nu(x)^T \beta$ and $\nu(x) \in \reals^p$ is a known function. Let $\{ (x^{(i)}, y^{(i)}) \}_{i=1, \ldots, n}$ be IID samples drawn from the joint distribution of $X$ and $Y$. Let $V$ be the matrix whose $i$th row is $\nu(x^{(i)})$, and assume $V$ is full rank. Let $\hat{\beta} = (V^T V)^{-1} V^T y$. Then $\hat{\beta} \sim \normal (\beta, \mathcal{I}^{-1})$, where $\mathcal{I} = (V^T V) / \sigma^2$ is the Fisher information matrix. Specifically, $\hat{\beta}$ has a multivariate normal distribution, and $\hat{\beta}$ is an unbiased estimate of $\beta$. It is also a \textit{consistent} estimate of $\beta$, meaning that $\hat{\beta}$ converges in probability to $\beta$, as $n$ increases without limit. It is also the best linear unbiased estimate of $\beta$: any other linear, unbiased estimates have higher variance than $\hat{\beta}$ \cite[\S~1.3.9]{Wood:2017}.

A simple modification permits the model to be much more flexible. Suppose that the $i$th observation has variance $\sigma^2/w^{(i)}$, where $w^{(i)}$ are  known, positive numbers. Let $U$ be the matrix whose $i$th row is $\sqrt{w^{(i)}}\nu(x^{(i)})$, and let $z^{(i)} = \sqrt{w^{(i)}} y^{(i)}$. Then the conclusions of this section are valid substituting $y \to z$ and $V \to U$. For example, $\hat{\beta} = (U^T U)^{-1} U^T z$ is the best linear unbiased estimate of $\beta$ \cite[\S~5.1]{Weisberg:2005}. When $w^{(i)} = 1$, we get the original results since $V=U$ and $y=z$. In what follows, we will proceed in terms of $V$ and $y$.

If $\sigma^2$ is unknown, it may be estimated from the data. Let
\begin{equation}
   \hat{\sigma}^2 = \frac{\| y - V \hat{\beta} \|_2^2}{n-p}.
\end{equation}
Then $(n-p) \hat{\sigma}^2 / \sigma^2 \sim \chi_{n-p}^2$, and $\hat{\beta}$ and $\hat{\sigma}^2$ are independent \cite[\S~3.4.4]{Weisberg:2005}. This indicates that $\hat{\sigma}^2$ is an unbiased estimate of $\sigma^2$. Suppose $\Prob\{ \chi_{n-p}^2 \in (\ell, u) \} = \alpha$; that is, $(\ell, u)$ is a confidence interval (at level $\alpha$) on a $\chi_{n-p}^2$ random variable. Then 
\begin{equation}
   \Prob \left\{ \sigma^2 \in \left( \frac{(n-p) \cdot \hat{\sigma}^2}{u}, \frac{(n-p) \cdot \hat{\sigma}^2}{\ell} \right) \right\} = \alpha.
\end{equation}
A particularly useful case is when $u \to \infty$, corresponding to $\ell = \Phi_{\chi_{n-p}^2}^{-1}(1 - \alpha)$, where $\Phi_{\chi_{n-p}^2}$ is the cumulative distribution function of a $\chi_{n-p}^2$ random variable, in which case 
\begin{displaymath}
   \Prob\left\{\sigma^2 < \frac{(n-p) \cdot \hat{\sigma}^2}{\Phi_{\chi_{n-p}^2}^{-1}(1 - \alpha)} \right\} = \alpha,
\end{displaymath}
corresponding to an upper confidence limit on $\sigma^2$, at level $\alpha$.

We may compute confidence intervals on linear combinations of the components of $\beta$ by noting that $c^T \hat{\beta} \sim \normal (c^T \beta, \| c \|_\mathcal{I}^2)$, where $\| c \|_\mathcal{I}^2 = c^T \mathcal{I}^{-1} c$ is (the square of) the Mahalanobis norm \cite[\S~3.11.1]{Seber:2003}, and thus
\begin{equation}
\label{eqn:dist_ctbeta}
   \frac{c^T \hat{\beta} - c^T \beta}{\|c\|_\mathcal{I}} \sim \normal(0, 1).
\end{equation}
Let $z^{1-\alpha/2}$ be the upper $\alpha/2$ quantile of a standard Gaussian random variable. Then
\begin{equation}
\label{eqn:ci_beta}
c^T \hat{\beta} \pm z^{1-\alpha/2} \cdot \|c\|_\mathcal{I}
\end{equation}
are the endpoints of a $100(1-\alpha)\%$ confidence interval on $c^T \beta$. This formula is only computable when $\mathcal{I}$ is computable, which in turn is only possible when $\sigma^2$ is known a priori. When $\sigma^2$ is unknown, we need a formula in terms of its estimated value, $\hat{\sigma}^2$, leading to
\begin{displaymath}
   \frac{c^T \hat{\beta} - c^T \beta}{\| c \|_{\hat{\mathcal{I}}}} \sim t_{n-p},
\end{displaymath}
where $\hat{\mathcal{I}} = V^T V / \hat{\sigma}^2$ is the estimated Fisher information matrix. Thus, when $\sigma^2$ is unknown, a $100(1-\alpha)\%$ confidence interval on $c^T \beta$ has endpoints
\begin{equation}
\label{eqn:ci_beta_estimated_variance}
c^T \hat{\beta} \pm t_{n-p}^{1-\alpha/2} \cdot \| c \|_{\hat{\mathcal{I}}},
\end{equation}
where $t_{n-p}^{1-\alpha/2}$ is the upper $\alpha/2$ quantile of a Student's $t$ distribution with $n-p$ degrees of freedom.

These facts can be used for testing $H_0: c^T \beta = d$ vs.~the alternative, $H_1: c^T \beta \neq d$ for particular values of $c \in \reals^p$ and $d$. Under the null hypothesis,
\begin{equation}
\label{eqn:hts_olc}
   T_c := \frac{c^T \hat{\beta} - d}{\| c \|_{\hat{\mathcal{I}}}} \sim t_{n-p},
\end{equation}
so the p-value associated with the test is simply $\Phi_{t_{n-p}}(-|T_c|) + (1 - \Phi_{t_{n-p}}(|T_c|))$, where $\Phi_{t_{n-p}}$ is the cumulative distribution function for a Student's $t$ distribution with $n-p$ degrees of freedom. Under a particular alternative hypothesis, say $c^T \beta = d^\prime$, $T_c$ is distributed as a noncentral Student's $t$ with $n-p$ degrees of freedom and noncentrality parameter $\lambda = (d^\prime-d)/\| c \|_\mathcal{I}$. I could not find a derivation of this, so here is one:
\begin{displaymath}
   T_c = \frac{\frac{c^T \hat{\beta} - d^\prime}{\| c \|_\mathcal{I}} + \frac{d^\prime - d}{\| c \|_\mathcal{I}}}{\| c \|_{\hat{\mathcal{I}}} / \| c \|_\mathcal{I}}
   = \frac{\frac{c^T \hat{\beta} - d^\prime}{\| c \|_\mathcal{I}} + \frac{d^\prime - d}{\| c \|_\mathcal{I}}}{\sqrt{\hat{\sigma}^2 / \sigma^2}}
   = \frac{\frac{c^T \hat{\beta} - d^\prime}{\| c \|_\mathcal{I}} + \frac{d^\prime - d}{\| c \|_\mathcal{I}}}{\sqrt{\frac{(n-p) \cdot \hat{\sigma}^2 / \sigma^2}{n-p}}}.
\end{displaymath}
From Equation~(\ref{eqn:dist_ctbeta}), the first term in the numerator has a standard Gaussian distribution. The second term in the numerator is the noncentrality parameter. The denominator is the square root of a $\chi_{n-p}^2$ random variable divided by its degrees of freedom. The numerator is a function of $\hat{\beta}$ while the denominator is a function of $\hat{\sigma}^2$, so the numerator and denominator are statistically independent. This is precisely the characterization of a noncentral Student's $t$ distribution. For a test of size $\alpha$, we would reject the null if $|T_c| > t_{n-p}^{1-\alpha/2}$. The probability of doing so under a particular alternative hypothesis is the power of the test and is given by:
\begin{displaymath}
1 - \Phi_{t_{n-p; \lambda}}(t_{n-p}^{1-\alpha/2}) + \Phi_{t_{n-p; \lambda}}(-t_{n-p}^{1-\alpha/2}),
\end{displaymath}
where $\Phi_{t_{n-p; \lambda}}$ is the cumulative distribution function for a noncentral Student's $t$ distribution with $n-p$ degrees of freedom and noncentrality parameter $\lambda = (d^\prime-d)/\| c \|_\mathcal{I}$. Note we need to assume a value of $\sigma^2$ associated with the alternative hypothesis to compute $\| c \|_\mathcal{I}$ for the noncentrality parameter.

As special cases of the above discussion, when $c = \hat{e}_i$, we get confidence intervals for, and hypothesis tests regarding, $\beta_i$. When $c = \nu(x_\textrm{new})$, we get confidence intervals for $\mu(x_\textrm{new})$; that is, the mean response of the model applied to a new data point. Confidence intervals on $Y \given X = x_\textrm{new}$ involve an extra component of uncertainty due to the variance of the conditional distribution: even if we knew $\mu(x_\textrm{new})$ perfectly, the conditional distribution still has variance $\sigma^2$. This leads to an extra term of $\hat{\sigma}^2$ as compared to Equation~(\ref{eqn:ci_beta_estimated_variance}):
\begin{displaymath}
\nu(x_\textrm{new})^T \hat{\beta} \pm t_{n-p}^{1-\alpha/2} \cdot \sqrt{\hat{\sigma}^2 + \| \nu(x_\textrm{new}) \|_{\hat{\mathcal{I}}}^2},
\end{displaymath}
which are the endpoints of a confidence interval on $Y$ \cite[\S 3.6]{Weisberg:2005}.

When we want simultaneous confidence intervals on multiple linear combinations of $\beta$, we must adjust for the multiple comparisons. There is more than one approach to doing so. Suppose we are interested in $C \beta$, where $C \in \reals^{q \times p}$. Then
\begin{displaymath}
    C \hat{\beta} \sim \normal \left( C \beta, C \mathcal{I}^{-1} C^T \right),
\end{displaymath}
which shows that $C \hat{\beta}$ is an unbiased estimator for $C \beta$, and that $C \hat{\beta}$ is normally distributed. Note that when $C = V$, we get the distribution of the fitted means, $\hat{\mu}$, since $\hat{\mu} := V \hat{\beta}$. Suppose we are interested in simultaneous confidence intervals on $C\beta$ at level $\alpha$. The Bonferroni correction defines $\alpha^\prime = \alpha / q$ and then simply uses the endpoints in Equation~(\ref{eqn:ci_beta_estimated_variance}) for each individual component of $C\beta$, substituting $\alpha^\prime$ for $\alpha$. For example, suppose we wanted a simultaneous $95\%$ confidence interval on $C\beta$, where $C$ has $q=5$ rows. Then $\alpha = 0.05$ and $\alpha^\prime = 0.01$. So we would compute $99\%$ confidence intervals on each component $c^{(i)} \beta$, where $c^{(i)}$ is the $i$th row of $C$. This approach is simple but overly conservative in many cases \cite[\S~9.1.3]{Weisberg:2005}.

Another method, due to Scheff\'e, defines simultaneous confidence intervals for \textit{any} linear function of $C\beta$. This is especially helpful when $q$ is very large (in that case, the Bonferroni correction renders the confidence intervals too wide to be practically useful). Suppose $C$ has rank $r$, and that $A \in \reals^{r \times p}$ is any collection of $r$ linearly independent rows of $C$. We wish to estimate simultaneous confidence intervals on quantities of the form $h^T A \beta$.\footnote{Since the range of $A^T$ is equal to the range of $C^T$, for any vector $h^\prime \in \reals^q$, there exists $h \in \reals^r$ such that $C^T h^\prime = A^T h$.} For example, when $h = \hat{e}_i$, this is simply the $i$th component of $A \beta$, but $h$ can be anything. Then
\begin{displaymath}
   h^T A \hat{\beta} \pm (r \cdot F_{r, n-p}^{1-\alpha/2})^{1/2} \cdot \| A^T h \|_{\hat{\mathcal{I}}}
\end{displaymath}
is a $100(1-\alpha)\%$ confidence interval on $h^T A \beta$, where $F_{r, n-p}^{1-\alpha/2}$ is the upper $\alpha/2$ quantile of Snedecor's $F$ distribution having $r$ and $n-p$ degrees of freedom in the numerator and denominator, respectively \cite[\S~5.1.1]{Seber:2003}.

Now suppose $C$ is full rank, and that $q < p$. We would like to test the hypothesis $C \beta = d$. As derived in \cite[\S~1.3.4]{Wood:2017},
\begin{displaymath}
   T_C := \frac{1}{q} (C \hat{\beta} - d)^T (C \hat{\mathcal{I}}^{-1} C^T)^{-1} (C \hat{\beta} - d) = \frac{1}{q}\| C \hat{\beta} - d \|_{C \hat{\mathcal{I}}^{-1} C^T}^2 \sim F_{q, n-p},
\end{displaymath}
where $F_{q, n-p}$ is Snedecor's $F$ distribution. This relationship generalizes~(\ref{eqn:hts_olc}) since an $F$ distribution with one degree of freedom in the numerator is equivalent to a $t^2$ distribution \cite[\S~3.5.3]{Weisberg:2005}. The p-value for this test would be $\Phi_{F_{q, n-p}}(-|T_C|) + (1 - \Phi_{F_{q, n-p}}(|T_C|))$, where $\Phi_{F_{q, n-p}}$ is the cumulative distribution function for an $F$ distribution with the specified degrees of freedom.

Under a particular alternative hypothesis, say $C \hat{\beta} = d^\prime$, $T_C$ has a noncentral $F$ distribution with noncentrality parameter $\lambda = \| d^\prime - d \|_{C \mathcal{I}^{-1} C^T}^2$. As above, I cannot find the derivation of this anywhere, so I'll provide it here. Let $L^T L = (C \mathcal{I}^{-1} C^T)^{-1}$ (for example, $L$ is the Cholesky decomposition). Then $L (C \hat{\beta} - d^\prime) \sim \normal(0, I)$ and $L(C \hat{\beta} - d) \sim \normal(L (d^\prime - d), I)$. Under the alternative hypothesis,
\begin{align*}
   T_C &= \frac{1}{q} (C \hat{\beta} - d)^T (C \hat{\mathcal{I}}^{-1} C^T)^{-1} (C \hat{\beta} - d) \\
      &= \frac{\frac{(C \hat{\beta} - d)^T (C \mathcal{I}^{-1} C^T)^{-1} (C \hat{\beta} - d)}{q}}{\frac{(n-p) \hat{\sigma}^2 / \sigma^2}{n-p}} \\
      &= \frac{\frac{\| L (C \hat{\beta} - d) \|_2^2}{q}}{\frac{(n-p) \hat{\sigma}^2 / \sigma^2}{n-p}}.
\end{align*}
The denominator is a $\chi_{n-p}^2$ random variable divided by its degrees of freedom. The numerator is the sum of squares of independent unit variance Gaussian random variables with mean vector $\mu = L (d^\prime - d)$, so letting 
\begin{align*}
   \lambda &= \mu^T \mu \\
      &= (d^\prime - d)^T L^T L (d^\prime - d) \\
      &= (d^\prime - d)^T (C \mathcal{I}^{-1} C^T)^{-1} (d^\prime - d) \\
      &= \| d^\prime - d \|_{C \mathcal{I}^{-1} C^T}^2,
\end{align*}
we see that the numerator is a noncentral $\chi_q^2$ random variable with noncentrality parameter $\lambda$, divided by its degrees of freedom. Since the numerator is a function of $\hat{\beta}$, and the denominator is a function of $\hat{\sigma}^2$, the numerator and denominator are statistically independent, which demonstrates the test statistic is $F$-distributed. For a test of size $\alpha$, we would reject the null if $|T_C| > F_{q, n-p}^{1 - \alpha/2}$. The probability of doing so under a particular hypothesis is the power of the test and is given by:
\begin{displaymath}
   1 - \Phi_{F_{q, n-p; \lambda}}(F_{q, n-p}^{1 - \alpha/2}) + \Phi_{F_{q, n-p; \lambda}}(-F_{q, n-p}^{1 - \alpha/2}),
\end{displaymath}
where $\Phi_{F_{q, n-p; \lambda}}$ is the cumulative distribution function of a noncentral $F$ distribution with the stated degrees of freedom and noncentrality parameter.

The $F$-test is useful in several situations. Suppose we are considering a sequence of nested models: a model consisting only of a grand mean (in which $\mu$ does not depend on the features at all), a model consisting only of main effects, a model with first-order interactions, and a model with higher-order interactions. We may wish to check $H_0$: the model consists only of main effects vs. $H_1$: interactions are present. This amounts to checking whether $C \beta = 0$, where $C \beta$ are the components of $\beta$ corresponding to the interaction terms. Or we may want to check $H_0: \mu(x) = \mu$ vs. $H_1: \mu(x) \neq \mu$, where $\mu$ is the grand mean. In this case, we are checking whether there is any evidence of the mean response depending on the features. Of course, checking multiple hypotheses is subject to the multiple comparisons problem discussed above.

If a particular feature is categorical with at least three levels, it will consist of at least two parameters. We would typically want to test whether all associated parameters are non-zero, not just one. Or if $\nu(x)$ consists of multiple transformations of a particular feature, like $\nu(x) = \begin{bmatrix} \cdots & x_1 & x_1^2 & \log(x_1) & \cdots \end{bmatrix}^T$, we might want to simultaneously test all the corresponding components of $\beta$ for being non-zero. The $F$-test described here is applicable to these scenarios.

The $F$-test is not the only approach to model selection, however; other methods include those based on information criteria like Akaike's Information Criterion (AIC) \cite[\S~12.3.3]{Seber:2003}, the Bayesian Information Criterion (BIC), and Mallow's $C_p$ statistic \cite[\S~10.2.1]{Weisberg:2005}. These are given, respectively, by:
\begin{align*}
   \textrm{AIC} &= n \log (2 \pi \sigma^2) + \frac{1}{\sigma^2} \| y - V \hat{\beta} \|_2^2 + 2 \cdot \textrm{dof} \quad \textrm{($\sigma^2$ known)}\\
    \textrm{AIC} &= n \log (2 \pi \| y - V \hat{\beta} \|_2^2 / n) + n + 2 \cdot \textrm{dof} \quad \textrm{($\sigma^2$ unknown)} \\
     \textrm{AICc} &= \textrm{AIC} + \frac{2\textrm{dof}^2 + 2 \textrm{dof}}{n - \textrm{dof} - 1} \quad \textrm{(linear model correct)} \\
      &= n \log (2 \pi \| y - V \hat{\beta} \|_2^2 / n) + \frac{n(n + \textrm{dof} - 1) }{n - \textrm{dof} - 1} \quad \textrm{($\sigma^2$ unknown)} \\
   \textrm{BIC} &= n \log(\| y - V \hat{\beta} \|_2^2 / n) + \textrm{dof} \cdot \log(n) \\
   C_p &= \frac{\| y - V \hat{\beta} \|_2^2}{\sigma^2}  + 2 \cdot \textrm{dof} - n,
\end{align*}
where dof is equal to the number of parameters estimated in the model. If $\sigma^2$ is known a priori, this is simply $p$; otherwise, it is $p+1$. Note that AIC comes in two forms depending on whether $\sigma^2$ is known a priori. A modification of AIC is often desirable for small sample sizes; this is known as the corrected AIC, or AICc. The correction term depends on whether we believe the model truly is normally distributed with a mean depending linearly on the parameters; this is the formula shown \cite[\S~7.7.6]{Burnham:2002}.

These formulae clearly illustrate the tradeoff between a better fitting model and a model having more parameters. Notably, the BIC formula penalizes degrees of freedom much more strongly than does the AIC, and thus will lead to simpler models. Caution is advised when using $C_p$ with unknown $\sigma^2$ \cite[\S~1.8.6]{Wood:2017}. If we must, it is best to estimate $\sigma^2$ using the most flexible model under consideration (that is, the model with all parameters included), and using the same value for all models being compared.

Cross validation is another, more computationally intensive approach to model selection. By dividing the data set into a training and test set, we fit the model to the training set and use the result to predict the response for the data in the test set, using the actual response to compute the prediction error. The model giving the best prediction error is the one we select. Often we will then refit the model on the entire data set \cite[\S~7.10]{Hastie:2001}.

Finally, we discuss the model residuals, $\hat{\epsilon}^{(i)} = y^{(i)} - \hat{\mu}(x^{(i)})$. We have already been using the residuals to estimate the variance, $\sigma^2$, but examining the residuals is also useful for investigating departures from the assumptions of the linear model: that the response is normally distributed, that the mean of this distribution depends linearly on the model parameters, that the variance is constant, and that the observations are statistically independent. These assumptions may be checked by graphing the residuals.

Let $H = V (V^T V)^{-1} V$ be the so-called \textit{hat matrix}. Then $\hat{\epsilon} \sim \normal(0, \sigma^2 (I - H))$. Notably, the residuals are correlated, and have different variances. Because of this, it is typical to standardize the residuals so that they have equal variance. The internally and externally Studentized residuals are defined as
\begin{align*}
   r^{(i)} &= \frac{\hat{\epsilon}^{(i)}}{\sqrt{\hat{\sigma}^2 \cdot (1 - h_i)}} \\
   t^{(i)} &= \frac{\hat{\epsilon}^{(i)}}{\sqrt{\hat{\sigma}_{(i)}^2 \cdot (1 - h_i)}},
\end{align*}
respectively, where $h_i$ is the $i$th diagonal element of $H$ and $\hat{\sigma}_{(i)}^2 = \frac{1}{n-p-1} \sum_{j \neq i} \hat{\epsilon}^{(j)}$. As \cite[\S~10.2]{Seber:2003} states, $(r^{(i)})^2/(n-p)$ has a $\textrm{beta}[\frac{1}{2}, \frac{1}{2}(n-p-1)]$ distribution which means they are identically distributed (but not independent). The externally Studentized residuals, $t^{(i)}$, have a $t_{n-p-1}$ distribution. The externally Studentized residuals are less prone to outliers than are the internally Studentized residuals.

Consider a graph of $t^{(i)}$ (or $r^{(i)}$) against $\hat{\mu}(x^{(i)})$. If the model is correct, we expect to see a scattering of points with no discernible pattern, since the Studentized residuals would be identically distributed. If there is an apparent trend in the residuals, that would indicate a nonlinearity in the model. Or if the variance of the residuals seems to depend on the mean response, that would call into question the constant variance assumption \cite[\S~8.1.5]{Weisberg:2005}.

Since we know the theoretical distribution of the Studentized residuals, we may compare the observed quantiles with the quantiles of their theoretical distributions. These are called Q-Q plots.

What about the residuals, $y^{(i)} - \hat{\mu}^{(i)}$? What do we do with the residuals (graph them against the mean response, make a Q-Q plot, standardize them, etc.)? What plots are useful? Checking for outliers, points with high leverage.


\newpage
\bibliography{gamdist}

\end{document}
