\documentclass[12pt]{article}
\usepackage{fullpage,graphicx,psfrag,amsmath,amsfonts,verbatim}
\usepackage[small,bf]{caption}

\input defs.tex

\newcommand{\gamdist}{\texttt{gamdist}}
\DeclareMathOperator{\Exp}{\operatorname{E}}
\DeclareMathOperator{\Var}{\operatorname{Var}}

\bibliographystyle{alpha}

\title{gamdist: Generalized Additive Models in Python}
\author{Bob Wilson}

\begin{document}
\maketitle

\begin{abstract}
TBD
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}
This paper introduces a Python library called \gamdist{}, which uses a distributed optimization technique called the Alternating Direction Method of Multipliers (ADMM) to fit a special type of regression model called a Generalized Additve Model (GAM)  to data. 

\paragraph{Outline of Paper} In \S\ref{sec:gam} we describe Generalized Additive Models. In \S\ref{sec:admm} we describe the Alternating Direction Method of Multipliers and how it may be used to fit GAMs. In \S\ref{sec:arch}, we describe the architecture of the library, including relevant implementation details.

\section{Generalized Additive Models}
\label{sec:gam}

The primary goal of \gamdist{} is the estimation of certain aspects of the joint distribution of a collection of one or more random variables $X$ called \textit{features} and a random variable $Y$ we will call the \textit{response}. Specifically, we are interested in the conditional distribution of $Y \given X$. We base our conclusions on a collection of observations $\{ (x^{(i)}, y^{(i)}) \}_{i=1, \dots, n}$ drawn IID from the joint distribution of $X$ and $Y$. Actually, under certain circumstances the observations may be drawn from a distribution different than the one we are attempting to understand. This is a point that does not receive enough attention in books on regression, so we will quickly give an overview.

Suppose the joint distribution of $X$ and $Y$ is $\mathcal{F}$, with density $f_{X, Y}(x, y)$. Denote by $f_X(x)$ the marginal density of $X$ obtained by integrating the joint density over $Y$. Suppose $f_X^\prime(x)$ is any density function with the same support as $f_X$. Define $f_{X, Y}^\prime (x, y) = f_{X, Y}(x, y) \cdot \frac{f_X^\prime(x)}{f_X(x)}$. Integrating both sides over $Y$ and then $X$ shows that $f_{X, Y}^\prime$ is a valid probability density function, corresponding to a distribution $\mathcal{F}^\prime$. The conditional distribution of $Y \given X$ is the same for both distributions since
\begin{align*}
   f(Y \given X) &= \frac{f(X, Y)}{f(X)} = \frac{f^\prime(X, Y)}{f^\prime(X)} = f^\prime(Y \given X).
\end{align*}
Whether we observe IID samples from $\mathcal{F}$ or $\mathcal{F}^\prime$, we estimate the same conditional distribution. For this reason, we say that $\mathcal{F}$ and $\mathcal{F}^\prime$ are \emph{compatible}. This fact may be exploited to provide greater precision in regions where $f_X(x)$ is small; by over-sampling in this region (choosing $f_X^\prime(x) \gg f_X(x)$) we can obtain greater precision. In fact, we may wish to choose $f_X^\prime$ to be fairly uniform over a region of interest to provide consistently high precision throughout. In an experimental setting, where we can choose the sampling mechanism, this is a powerful concept. We still must convince ourselves that the distribution from which we are sampling is indeed compatible with the distribution we want to learn about. In general, there is no reason to believe the distributions from which we draw our observations and on which we make our predictions are compatible!

We now discuss the linear model, which assumes $Y \given X = x \sim \normal (\mu(x), \sigma^2)$, where $\mu(x) = \nu(x)^T \beta$. This notation captures three key assumptions of the linear model. First, the conditional distribution is Gaussian for all values of $X$. Second, the mean of the distribution depends on the features $X$ in a fairly specific way discussed below. Third, the variance is the same for all values of $X$. These assumptions are all loosened in various generalizations of the linear model used in \gamdist{}.

If we choose $\nu(x) = x$, then the assumption is that $\mu(x) = x^T \beta$, and the mean depends linearly on the features; however, the \textit{linear} in \textit{linear model} refers to the dependence on $\beta$, not on the features. It is common to include a constant term in $\nu(x)$ to account for an affine dependency between the features and the response. For example, $\nu(x) = \begin{bmatrix} 1 & x_1 & x_2 & \cdots \end{bmatrix}^T$. We might incorporate quadratic terms to capture nonlinear dependencies including interactions, such as $\nu(x) = \begin{bmatrix}x_1 & x_2 & x_1^2 & x_1 \cdot x_2 & x_2^2 & \cdots \end{bmatrix}^T$. Or we might include more exotic transformations of the features, like $\nu(x) = \begin{bmatrix} \log(x_1) & \sin(x_2) & \cdots \end{bmatrix}^T$. These models are nonlinear in the features, but linear in the parameters $\beta$; however, the linear model will only incorporate transformations that we explicitly include. Since $\nu$ is more than just the feature vector we call it the \emph{design function}.

Another common situation is when some or all of the features are categorical. For example, consider a model with a single feature corresponding to a person's favorite color, and suppose choices are limited to red, green, and blue. The model would consist of the average responses for people who prefer any particular color. We might support such a model by defining
\begin{displaymath}
   \nu(x) = \begin{cases}
       \begin{bmatrix} 1 & 0 & 0 \end{bmatrix}^T & \textrm{if $x = $ red} \\
       \begin{bmatrix} 0 & 1 & 0 \end{bmatrix}^T & \textrm{if $x = $ green} \\
       \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}^T & \textrm{if $x = $ blue.}
   \end{cases}
\end{displaymath}
Alternative approaches to encoding categorical variables are common and useful in different circumstances. We see that the simple linear model is applicable to a wide range of problems, even those that may not appear linear at first glance.

Fitting a linear model to a set of observations is called linear regression, and is accomplished by solving a least squares optimization problem. This is an example of maximum likelihood estimation (MLE), itself a special case of maximum a posteriori (MAP) estimation, which is the unifying approach used throughout \gamdist{}. It is worth formulating this optimization problem so that we may see how it evolves as we consider more general scenarios.

Recall that we have $n$ observations of the form $\{ (x^{(i)}, \,y^{(i)} ) \}_{i=1, \ldots, n}$ drawn IID from a distribution compatible with the distribution of interest.\footnote{This assumption is loosened in random effects or mixed effects models which are not considered here; see \cite{Stroup:2012}.} Under the assumptions of the linear model, $Y \given X = x^{(i)} \sim \normal (\mu(x^{(i)}), \sigma^2)$. The likelihood of a particular observation is
\begin{displaymath}
    \mathcal{L}(\beta; x^{(i)}, y^{(i)}) = \frac{1}{\sqrt{2\pi \sigma^2}} \cdot \exp\left( -\frac{1}{2 \sigma^2} \left(y^{(i)} - \nu \left( x^{(i)} \right)^T \beta \right)^2 \right).
\end{displaymath}
Note that by convention the likelihood is interpreted as a function of $\beta$ parameterized by the observation $(x^{(i)}, y^{(i)})$. The likelihood of the entire set of observations is the product of the likelihoods of the individual observations: $\mathcal{L}(\beta; x, y) = \Pi_{i=1}^n \mathcal{L}(\beta; x^{(i)}, y^{(i)})$. The log-likelihood is the sum of the log-likelihoods of the individual observations:
\begin{displaymath}
   \ell(\beta; x, y) = \log \mathcal{L}(\beta; x, y) = -n/2 \log(2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n \left( y^{(i)} - \nu \left(x^{(i)} \right)^T \beta \right)^2.
\end{displaymath}
Maximizing the likelihood is the same as maximizing the log-likelihood, and if we are only interested in estimating $\beta$, this is equivalent to the problem
\begin{displaymath}
        \begin{array}{ll}
            \mbox{minimize} & {\displaystyle \sum_{i=1}^n} \left( y^{(i)} - \nu \left(x^{(i)} \right)^T \beta \right)^2,
        \end{array}
\end{displaymath}
where the variable is $\beta$ and $y^{(i)}$ and $\nu(x^{(i)})$ are data. An elementary result in optimization theory is that a unique solution exists if and only if $V^T V$ is full rank, where the $i$th row of $V$ is equal to $\nu(x^{(i)})^T$. In that case, the optimal $\beta$ satisfies the so-called normal equations:
\begin{displaymath}
    V^T V \hat{\beta} = V^T  y.
\end{displaymath}

If we assume the model is correct (that is, that the conditional distribution really has the assumed form), exact formulae exist for confidence intervals on the parameters $\beta$. If the variance is unknown, it too can be estimated from the data. We can employ hypothesis tests against the null hypothesis that some or all of the components of $\beta$ are zero. We can apply the resulting model to new data assumed to be drawn from the same joint distribution to compute confidence intervals on the response, $Y \given X = x_\textrm{new}$ or the mean of this distribution, $\mu(x_\textrm{new})$. These are immensely valuable tools in the analysis of data and the application of data to predictions. A good reference on linear models is \cite{Weisberg:2005}. Useful results are collected in Appendix~\ref{app:linear_model}.

Even if the assumptions underlying the linear model are correct, if the noise is high, or if the number of features is large relative to the number of observations, we may use regularization to improve both the estimates of $\beta$ and predictions based on the estimated model. Regularization reduces the sensitivity of the estimates to noise at the expense of introducing bias, and may be thought of as imposing a Bayesian prior on the parameters $\beta$. Some of the most common forms of regularization include ridge regression and the lasso \cite{Tibs:96}. For example, the lasso may be formulated as the problem:
\begin{displaymath}
        \begin{array}{ll}
            \mbox{minimize} & {\displaystyle \sum_{i=1}^n} \left( y^{(i)} - \nu \left(x^{(i)} \right)^T \beta \right)^2 + \lambda \cdot \| \beta \|_1,
        \end{array}
\end{displaymath}
but this problem does not have a closed-form solution. Moreover, introducing regularization means the formulation is no longer a maximum likelihood estimation problem. Instead, it is a maximum a posteriori estimation problem. MLE problems have some statistical properties that MAP estimation problems do not possess.

If the conditional distribution is not Gaussian, other techniques may prove more useful. For example, if the conditional distribution is Laplacian, we may use least absolute deviation regression instead of least squares \cite{Birkes:1993}. Like with the lasso, there are no exact formulae for statistical inference in this context and we must settle for an asymptotic or non-parametric approach such as the bootstrap \cite{Efron:1993}.

Yet another approach to extending linear models was introduced by \cite{NW:72} and discussed in detail in \cite{MN:1983}. Their formulation extends the linear model in a few ways. The conditional distribution is not assumed to be Gaussian. Common alternatives include the binomial and Poisson distributions. The mean of the distribution is permitted to depend on the features in a more complicated way, via the introduction of a \textit{link function}, $g$: $g(\mu(x)) = \eta(x) = \nu(x)^T \beta$. When $g(x) = x$, this recovers the same relationship between the features and $\mu$ assumed in the linear model, but other link functions may be used like the logistic function $g(x) = \log(x / (1-x))$. Finally, the variance is sometimes permitted to depend on $x$ instead of being constant. Such models are called Generalized Linear Models (GLMs). Fitting such models is accomplished via maximum likelihood estimation:
\begin{displaymath}
        \begin{array}{ll}
            \mbox{minimize} & {\displaystyle \sum_{i=1}^n} \ell\left( \beta; x^{(i)}, y^{(i)}\right).
        \end{array}
\end{displaymath}
Regularization may be added to the objective term just as in linear regression, but then this is no longer a MLE problem. For many choices of distribution family, link function, and regularization, the corresponding optimization problem is convex. Appendix~\ref{app:glm} collects some useful results about GLMs.

Introduced by \cite{HT:86}, Generalized Additive Models (GAMs) extend GLMs by permitting $\eta(x)$ to be a nonparametric function of the features: $\eta(x) = \sum_{i=1}^p h_i (x_i)$, where $h_i$ are smooth functions. When $h_i(x_i) = \beta_i x_i$, the linear model is recovered (all of the parametric dependencies discussed with regards to $\nu$  are still possible here of course, but the idea is that the data itself should tell us the form of the relationship). Typically the $h_i$ functions are chosen to be some sort of spline, such as a natural cubic spline. Yet again, GAMs are fit using MAP estimation, and when the functions $h_i$ are chosen to be natural cubic splines, this corresponds to a convex optimization problem. The full optimization problem may be formulated as:
\begin{displaymath}
        \begin{array}{ll}
            \mbox{minimize} & {\displaystyle \sum_{i=1}^n} \ell\left( \beta; x^{(i)}, y^{(i)}\right) + r(\beta),
        \end{array}
\end{displaymath}
where $r$ is a regularization function applied to the parameters $\beta$ alone (and not to the data).


\section{The Alternating Direction Method of Multipliers}
\label{sec:admm}

\section{Software Architecture}
\label{sec:arch}

\section*{Acknowledgments}

This is an example of an unnumbered section.

\appendix

\section{Properties of the Linear Model}
\label{app:linear_model}

In this section, we state various useful facts (without proof) regarding the linear model. For details, see \cite{Weisberg:2005}, \cite{Seber:2003}, \cite{Wood:2017}, and \cite{CB:2001}. The goal of this section is twofold: to gather formulae used in \gamdist{}, and to highlight what we typically want to do in a regression analysis, beyond just fitting the model. In fact, it is often desirable to quantify the uncertainty in fitted model parameters, check the assumptions of the model through examination of the residuals, perform model selection, and quantify the uncertainty associated with predictions.

The beauty of the linear model is that exact formulae are attainable in support of these goals. Other types of regression only support asymptotic or approximate formulae. However, it is rarely the case that the assumptions of the linear model are expected to hold exactly. If the assumptions are approximately valid, we may hope the formulae which follow are approximately valid. Much has been written about the robustness of these formulae under deviations from the assumptions \cite[\S 9]{Seber:2003}. We are inspired by the maxim \cite{Box:1987}, ``All models are wrong, but some are useful."

\subsection{Properties of the Estimated Model}
Suppose $Y \given X = x \sim \normal (\mu(x), \sigma^2)$, where $\mu(x) = \nu(x)^T \beta$ and $\nu(x) \in \reals^p$ is a known function. Let $\{ (x^{(i)}, y^{(i)}) \}_{i=1, \ldots, n}$ be IID samples drawn from the joint distribution of $X$ and $Y$. Let $V$ be the matrix whose $i$th row is $\nu(x^{(i)})$, and assume $V$ is full rank. Let $\hat{\beta} = (V^T V)^{-1} V^T y$. Then $\hat{\beta} \sim \normal (\beta, \mathcal{I}^{-1})$, where $\mathcal{I} = (V^T V) / \sigma^2$ is the Fisher information matrix. Specifically, $\hat{\beta}$ has a multivariate normal distribution, and $\hat{\beta}$ is an unbiased estimate of $\beta$. It is also a \textit{consistent} estimate of $\beta$, meaning that $\hat{\beta}$ converges in probability to $\beta$, as $n$ increases without limit. It is also the best linear unbiased estimate of $\beta$: any other linear, unbiased estimates have higher variance than $\hat{\beta}$ \cite[\S~1.3.9]{Wood:2017}.

A simple modification permits the model to be much more flexible. Suppose that the $i$th observation has variance $\sigma^2/w^{(i)}$, where $w^{(i)}$ are  known, positive numbers. Let $U$ be the matrix whose $i$th row is $\sqrt{w^{(i)}}\nu(x^{(i)})$, and let $z^{(i)} = \sqrt{w^{(i)}} y^{(i)}$. Then the conclusions of this section are valid substituting $y \to z$ and $V \to U$. For example, $\hat{\beta} = (U^T U)^{-1} U^T z$ is the best linear unbiased estimate of $\beta$ \cite[\S~5.1]{Weisberg:2005}. When $w^{(i)} = 1$, we get the original results since $V=U$ and $y=z$. In what follows, we will proceed in terms of $V$ and $y$.

If $\sigma^2$ is unknown, it may be estimated from the data. Let
\begin{equation}
   \hat{\sigma}^2 = \frac{\| y - V \hat{\beta} \|_2^2}{n-p}.
\end{equation}
Then $(n-p) \hat{\sigma}^2 / \sigma^2 \sim \chi_{n-p}^2$, and $\hat{\beta}$ and $\hat{\sigma}^2$ are independent \cite[\S~3.4.4]{Weisberg:2005}. This indicates that $\hat{\sigma}^2$ is an unbiased estimate of $\sigma^2$.

\subsection{Confidence Intervals and Hypothesis Tests \label{sec:CIHT}}
Suppose $\Prob\{ \chi_{n-p}^2 \in (\ell, u) \} = \alpha$; that is, $(\ell, u)$ is a confidence interval (at level $\alpha$) on a $\chi_{n-p}^2$ random variable. Then 
\begin{equation}\label{eqn:var_ci}
   \Prob \left\{ \sigma^2 \in \left( \frac{(n-p) \cdot \hat{\sigma}^2}{u}, \frac{(n-p) \cdot \hat{\sigma}^2}{\ell} \right) \right\} = \alpha.
\end{equation}
A particularly useful case is when $u \to \infty$, corresponding to $\ell = \Phi_{\chi_{n-p}^2}^{-1}(1 - \alpha)$, where $\Phi_{\chi_{n-p}^2}$ is the cumulative distribution function of a $\chi_{n-p}^2$ random variable, in which case 
\begin{displaymath}
   \Prob\left\{\sigma^2 < \frac{(n-p) \cdot \hat{\sigma}^2}{\Phi_{\chi_{n-p}^2}^{-1}(1 - \alpha)} \right\} = \alpha,
\end{displaymath}
corresponding to an upper confidence limit on $\sigma^2$, at level $\alpha$.

We may compute confidence intervals on linear combinations of the components of $\beta$ by noting that $c^T \hat{\beta} \sim \normal (c^T \beta, \| c \|_\mathcal{I}^2)$, where $\| c \|_\mathcal{I}^2 = c^T \mathcal{I}^{-1} c$ is (the square of) the Mahalanobis norm \cite[\S~3.11.1]{Seber:2003}, and thus
\begin{equation}
\label{eqn:dist_ctbeta}
   \frac{c^T \hat{\beta} - c^T \beta}{\|c\|_\mathcal{I}} \sim \normal(0, 1).
\end{equation}
Let $z^{1-\alpha/2}$ be the upper $\alpha/2$ quantile of a standard Gaussian random variable. Then
\begin{equation}
\label{eqn:ci_beta}
c^T \hat{\beta} \pm z^{1-\alpha/2} \cdot \|c\|_\mathcal{I}
\end{equation}
are the endpoints of a $100(1-\alpha)\%$ confidence interval on $c^T \beta$. This formula is only computable when $\mathcal{I}$ is computable, which in turn is only possible when $\sigma^2$ is known a priori. When $\sigma^2$ is unknown, we need a formula in terms of its estimated value, $\hat{\sigma}^2$, leading to
\begin{displaymath}
   \frac{c^T \hat{\beta} - c^T \beta}{\| c \|_{\hat{\mathcal{I}}}} \sim t_{n-p},
\end{displaymath}
where $\hat{\mathcal{I}} = V^T V / \hat{\sigma}^2$ is the estimated Fisher information matrix. Thus, when $\sigma^2$ is unknown, a $100(1-\alpha)\%$ confidence interval on $c^T \beta$ has endpoints
\begin{equation}
\label{eqn:ci_beta_estimated_variance}
c^T \hat{\beta} \pm t_{n-p}^{1-\alpha/2} \cdot \| c \|_{\hat{\mathcal{I}}},
\end{equation}
where $t_{n-p}^{1-\alpha/2}$ is the upper $\alpha/2$ quantile of a Student's $t$ distribution with $n-p$ degrees of freedom.

These facts can be used for testing $H_0: c^T \beta = d$ vs.~the alternative, $H_1: c^T \beta \neq d$ for particular values of $c \in \reals^p$ and $d$. Under the null hypothesis,
\begin{equation}
\label{eqn:hts_olc}
   T_c := \frac{c^T \hat{\beta} - d}{\| c \|_{\hat{\mathcal{I}}}} \sim t_{n-p},
\end{equation}
so the p-value associated with the test is simply $\Phi_{t_{n-p}}(-|T_c|) + (1 - \Phi_{t_{n-p}}(|T_c|))$, where $\Phi_{t_{n-p}}$ is the cumulative distribution function for a Student's $t$ distribution with $n-p$ degrees of freedom. Under a particular alternative hypothesis, say $c^T \beta = d^\prime$, $T_c$ is distributed as a noncentral Student's $t$ with $n-p$ degrees of freedom and noncentrality parameter $\lambda = (d^\prime-d)/\| c \|_\mathcal{I}$. I could not find a derivation of this, so here is one:
\begin{displaymath}
   T_c = \frac{\frac{c^T \hat{\beta} - d^\prime}{\| c \|_\mathcal{I}} + \frac{d^\prime - d}{\| c \|_\mathcal{I}}}{\| c \|_{\hat{\mathcal{I}}} / \| c \|_\mathcal{I}}
   = \frac{\frac{c^T \hat{\beta} - d^\prime}{\| c \|_\mathcal{I}} + \frac{d^\prime - d}{\| c \|_\mathcal{I}}}{\sqrt{\hat{\sigma}^2 / \sigma^2}}
   = \frac{\frac{c^T \hat{\beta} - d^\prime}{\| c \|_\mathcal{I}} + \frac{d^\prime - d}{\| c \|_\mathcal{I}}}{\sqrt{\frac{(n-p) \cdot \hat{\sigma}^2 / \sigma^2}{n-p}}}.
\end{displaymath}
From Equation~(\ref{eqn:dist_ctbeta}), the first term in the numerator has a standard Gaussian distribution. The second term in the numerator is the noncentrality parameter. The denominator is the square root of a $\chi_{n-p}^2$ random variable divided by its degrees of freedom. The numerator is a function of $\hat{\beta}$ while the denominator is a function of $\hat{\sigma}^2$, so the numerator and denominator are statistically independent. This is precisely the characterization of a noncentral Student's $t$ distribution. For a test of size $\alpha$, we would reject the null if $|T_c| > t_{n-p}^{1-\alpha/2}$. The probability of doing so under a particular alternative hypothesis is the power of the test and is given by:
\begin{displaymath}
1 - \Phi_{t_{n-p; \lambda}}(t_{n-p}^{1-\alpha/2}) + \Phi_{t_{n-p; \lambda}}(-t_{n-p}^{1-\alpha/2}),
\end{displaymath}
where $\Phi_{t_{n-p; \lambda}}$ is the cumulative distribution function for a noncentral Student's $t$ distribution with $n-p$ degrees of freedom and noncentrality parameter $\lambda = (d^\prime-d)/\| c \|_\mathcal{I}$. Note we need to assume a value of $\sigma^2$ associated with the alternative hypothesis to compute $\| c \|_\mathcal{I}$ for the noncentrality parameter.

As special cases of the above discussion, when $c = \hat{e}_i$ (that is, a vector with a $1$ in the $i$th entry, and zeros elsewhere), we get confidence intervals for, and hypothesis tests regarding, $\beta_i$. When $c = \nu(x_\textrm{new})$, we get confidence intervals for $\mu(x_\textrm{new})$; that is, the mean response of the model applied to a new data point. Confidence intervals on $Y \given X = x_\textrm{new}$ involve an extra component of uncertainty due to the variance of the conditional distribution: even if we knew $\mu(x_\textrm{new})$ perfectly, the conditional distribution still has variance $\sigma^2$. This leads to an extra term of $\hat{\sigma}^2$ as compared to Equation~(\ref{eqn:ci_beta_estimated_variance}):
\begin{displaymath}
\nu(x_\textrm{new})^T \hat{\beta} \pm t_{n-p}^{1-\alpha/2} \cdot \sqrt{\hat{\sigma}^2 + \| \nu(x_\textrm{new}) \|_{\hat{\mathcal{I}}}^2},
\end{displaymath}
which are the endpoints of a confidence interval on $Y$ \cite[\S 3.6]{Weisberg:2005}.

When we want simultaneous confidence intervals on multiple linear combinations of $\beta$, we must adjust for the multiple comparisons. There is more than one approach to doing so. Suppose we are interested in $C \beta$, where $C \in \reals^{q \times p}$. Then
\begin{displaymath}
    C \hat{\beta} \sim \normal \left( C \beta, C \mathcal{I}^{-1} C^T \right),
\end{displaymath}
which shows that $C \hat{\beta}$ is an unbiased estimator for $C \beta$, and that $C \hat{\beta}$ is normally distributed. Note that when $C = V$, we get the distribution of the fitted means, $\hat{\mu}$, since $\hat{\mu} := V \hat{\beta}$. Suppose we are interested in simultaneous confidence intervals on $C\beta$ at level $\alpha$. The Bonferroni correction defines $\alpha^\prime = \alpha / q$ and then simply uses the endpoints in Equation~(\ref{eqn:ci_beta_estimated_variance}) for each individual component of $C\beta$, substituting $\alpha^\prime$ for $\alpha$. For example, suppose we wanted a simultaneous $95\%$ confidence interval on $C\beta$, where $C$ has $q=5$ rows. Then $\alpha = 0.05$ and $\alpha^\prime = 0.01$. So we would compute $99\%$ confidence intervals on each component $c^{(i)} \beta$, where $c^{(i)}$ is the $i$th row of $C$. This approach is simple but overly conservative in many cases \cite[\S~9.1.3]{Weisberg:2005}.

Another method, due to Scheff\'e, defines simultaneous confidence intervals for \textit{any} linear function of $C\beta$ \cite{Scheffe:1959}. This is especially helpful when $q$ is very large (in that case, the Bonferroni correction renders the confidence intervals too wide to be practically useful). Suppose $C$ has rank $r$, and that $A \in \reals^{r \times p}$ is any collection of $r$ linearly independent rows of $C$. We wish to estimate simultaneous confidence intervals on quantities of the form $h^T A \beta$.\footnote{Since the range of $A^T$ is equal to the range of $C^T$, for any vector $h^\prime \in \reals^q$, there exists $h \in \reals^r$ such that $C^T h^\prime = A^T h$.} For example, when $h = \hat{e}_i$, this is simply the $i$th component of $A \beta$, but $h$ can be anything. Then
\begin{displaymath}
   h^T A \hat{\beta} \pm (r \cdot F_{r, n-p}^{1-\alpha/2})^{1/2} \cdot \| A^T h \|_{\hat{\mathcal{I}}}
\end{displaymath}
is a $100(1-\alpha)\%$ confidence interval on $h^T A \beta$, where $F_{r, n-p}^{1-\alpha/2}$ is the upper $\alpha/2$ quantile of Snedecor's $F$ distribution having $r$ and $n-p$ degrees of freedom in the numerator and denominator, respectively \cite[\S~5.1.1]{Seber:2003}.

Now suppose $C$ is full rank, and that $q < p$. We would like to test the hypothesis $C \beta = d$. As derived in \cite[\S~1.3.4]{Wood:2017},
\begin{equation}\label{eqn:f_test}
   T_C(d) := \frac{1}{q} (C \hat{\beta} - d)^T (C \hat{\mathcal{I}}^{-1} C^T)^{-1} (C \hat{\beta} - d) = \frac{1}{q}\| C \hat{\beta} - d \|_{C \hat{\mathcal{I}}^{-1} C^T}^2 \sim F_{q, n-p},
\end{equation}
where $F_{q, n-p}$ is Snedecor's $F$ distribution. The notation is intended to make it clear that the test statistic depends on $d$; where there is no risk of confusion we will simply write $T_C$. This relationship generalizes~(\ref{eqn:hts_olc}) since an $F$ distribution with one degree of freedom in the numerator is equivalent to a $t^2$ distribution \cite[\S~3.5.3]{Weisberg:2005}. The p-value for this test would be $1 - \Phi_{F_{q, n-p}}(T_C)$, where $\Phi_{F_{q, n-p}}$ is the cumulative distribution function for an $F$ distribution with the specified degrees of freedom.

Under a particular alternative hypothesis, say $C \beta = d^\prime$, $T_C$ has a noncentral $F$ distribution with noncentrality parameter $\lambda = \| d^\prime - d \|_{C \mathcal{I}^{-1} C^T}^2$. As above, I cannot find the derivation of this anywhere, so I'll provide it here. Let $L^T L = (C \mathcal{I}^{-1} C^T)^{-1}$ (for example, $L$ is the Cholesky decomposition). Then $L (C \hat{\beta} - d^\prime) \sim \normal(0, I)$ and $L(C \hat{\beta} - d) \sim \normal(L (d^\prime - d), I)$. Under the alternative hypothesis,
\begin{align*}
   T_C &= \frac{1}{q} (C \hat{\beta} - d)^T (C \hat{\mathcal{I}}^{-1} C^T)^{-1} (C \hat{\beta} - d) \\
      &= \frac{\frac{(C \hat{\beta} - d)^T (C \mathcal{I}^{-1} C^T)^{-1} (C \hat{\beta} - d)}{q}}{\frac{(n-p) \hat{\sigma}^2 / \sigma^2}{n-p}} \\
      &= \frac{\frac{\| L (C \hat{\beta} - d) \|_2^2}{q}}{\frac{(n-p) \hat{\sigma}^2 / \sigma^2}{n-p}}.
\end{align*}
The denominator is a $\chi_{n-p}^2$ random variable divided by its degrees of freedom. The numerator is the sum of squares of independent unit variance Gaussian random variables with mean vector $\mu = L (d^\prime - d)$, so letting 
\begin{align*}
   \lambda &= \mu^T \mu \\
      &= (d^\prime - d)^T L^T L (d^\prime - d) \\
      &= (d^\prime - d)^T (C \mathcal{I}^{-1} C^T)^{-1} (d^\prime - d) \\
      &= \| d^\prime - d \|_{C \mathcal{I}^{-1} C^T}^2,
\end{align*}
we see that the numerator is a noncentral $\chi_q^2$ random variable with noncentrality parameter $\lambda$, divided by its degrees of freedom. Since the numerator is a function of $\hat{\beta}$, and the denominator is a function of $\hat{\sigma}^2$, the numerator and denominator are statistically independent, which demonstrates the test statistic is $F$-distributed. For a test of size $\alpha$, we would reject the null if $T_C > F_{q, n-p}^{1 - \alpha}$. The probability of doing so under a particular hypothesis is the power of the test and is given by:
\begin{displaymath}
   1 - \Phi_{F_{q, n-p; \lambda}}(F_{q, n-p}^{1 - \alpha}),
\end{displaymath}
where $\Phi_{F_{q, n-p; \lambda}}$ is the cumulative distribution function of a noncentral $F$ distribution with the stated degrees of freedom and noncentrality parameter.

Equation~(\ref{eqn:f_test}) enables us to compute a confidence region on $C \beta$. A $100(1-\alpha)\%$ confidence region is given by $\{ d \, : \, T_C(d) \leq F_{q, n-p}^{1 - \alpha} \}$. Since $T_C$ is a quadratic form, the confidence region is an ellipsoid centered at $C \hat{\beta}$ with orientation and size relating to the eigenvectors and eigenvalues of $C\mathcal{I}^{-1} C^T$, as well as the number of rows in $C$, q, and the confidence level $\alpha$. This confidence region is closely related to Scheff\'e's method.

\subsection{Model Selection}
The $F$-test is useful in several situations. Suppose we are considering a sequence of nested models: a model consisting only of a grand mean (in which $\mu$ does not depend on the features at all), a model consisting only of main effects, a model with first-order interactions, and a model with higher-order interactions. We may wish to check $H_0$: the model consists only of main effects vs. $H_1$: interactions are present. This amounts to checking whether $C \beta = 0$, where $C \beta$ are the components of $\beta$ corresponding to the interaction terms. Or we may want to check $H_0: \mu(x) = \mu$ vs. $H_1: \mu(x) \neq \mu$, where $\mu$ is the grand mean. In this case, we are checking whether there is any evidence of the mean response depending on the features. Of course, checking multiple hypotheses is subject to the multiple comparisons problem discussed above.

If a particular feature is categorical with at least three levels, it will consist of at least two parameters. We would typically want to test whether all associated parameters are non-zero, not just one. Or if $\nu(x)$ consists of multiple transformations of a particular feature, like $\nu(x) = \begin{bmatrix} \cdots & x_1 & x_1^2 & \log(x_1) & \cdots \end{bmatrix}^T$, we might want to simultaneously test all the corresponding components of $\beta$ for being non-zero. The $F$-test described here is applicable to these scenarios.

The $F$-test is not the only approach to model selection, however; other methods include those based on information criteria like Akaike's Information Criterion (AIC) \cite[\S~12.3.3]{Seber:2003}, the Bayesian Information Criterion (BIC), and Mallow's $C_p$ statistic \cite[\S~10.2.1]{Weisberg:2005}. These are given, respectively, by:
\begin{align*}
   \textrm{AIC} &= n \log (2 \pi \sigma^2) + \frac{1}{\sigma^2} \| y - V \hat{\beta} \|_2^2 + 2 \cdot \textrm{dof} \quad \textrm{($\sigma^2$ known)}\\
    \textrm{AIC} &= n \log (2 \pi \| y - V \hat{\beta} \|_2^2 / n) + n + 2 \cdot \textrm{dof} \quad \textrm{($\sigma^2$ unknown)} \\
     \textrm{AICc} &= \textrm{AIC} + \frac{2\textrm{dof}^2 + 2 \textrm{dof}}{n - \textrm{dof} - 1} \quad \textrm{(linear model correct)} \\
      &= n \log (2 \pi \| y - V \hat{\beta} \|_2^2 / n) + \frac{n(n + \textrm{dof} - 1) }{n - \textrm{dof} - 1} \quad \textrm{($\sigma^2$ unknown)} \\
   \textrm{BIC} &= n \log(\| y - V \hat{\beta} \|_2^2 / n) + \textrm{dof} \cdot \log(n) \\
   C_p &= \frac{\| y - V \hat{\beta} \|_2^2}{\sigma^2}  + 2 \cdot \textrm{dof} - n,
\end{align*}
where dof is equal to the number of parameters estimated in the model. If $\sigma^2$ is known a priori, this is simply $p$; otherwise, it is $p+1$. Note that AIC comes in two forms depending on whether $\sigma^2$ is known a priori. A modification of AIC is often desirable for small sample sizes; this is known as the corrected AIC, or AICc. The correction term depends on whether we believe the model truly is normally distributed with a mean depending linearly on the parameters; this is the formula shown \cite[\S~7.7.6]{Burnham:2002}.

These formulae clearly illustrate the tradeoff between a better fitting model and a model having more parameters. Notably, the BIC formula penalizes degrees of freedom much more strongly than does the AIC, and thus will lead to simpler models. Caution is advised when using $C_p$ with unknown $\sigma^2$ \cite[\S~1.8.6]{Wood:2017}. If we must, it is best to estimate $\sigma^2$ using the most flexible model under consideration (that is, the model with all parameters included), and using the same value for all models being compared.

Cross validation is another, more computationally intensive approach to model selection. By dividing the data set into a training and test set, we fit the model to the training set and use the result to predict the response for the data in the test set, using the actual response to compute the prediction error. The model giving the best prediction error is the one we select. Often we will then refit the model on the entire data set \cite[\S~7.10]{Hastie:2001}.

\subsection{Checking Model Assumptions}
Next, we discuss the model residuals, $\hat{\epsilon}^{(i)} = y^{(i)} - \hat{\mu}(x^{(i)})$. We have already been using the residuals to estimate the variance, $\sigma^2$, but examining the residuals is also useful for investigating departures from the assumptions of the linear model: that the response is normally distributed, that the mean of this distribution depends linearly on the model parameters, that the variance is constant (or is of the form $\sigma^2/w^{(i)}$ with known $w^{(i)}$), and that the observations are statistically independent. These assumptions may be checked by graphing the residuals.

Let $H = V (V^T V)^{-1} V^T$ be the so-called \textit{hat matrix}. Then $\hat{\epsilon} \sim \normal(0, \sigma^2 (I - H))$. Notably, the residuals are correlated, and have different variances (however, the residuals are statistically independent of $\hat{\mu}$). Because of this, it is typical to standardize the residuals so that they have equal variance. The internally and externally Studentized residuals are defined as
\begin{align*}
   r^{(i)} &= \frac{\hat{\epsilon}^{(i)}}{\sqrt{\hat{\sigma}^2 \cdot (1 - h_i)}} \\
   t^{(i)} &= \frac{\hat{\epsilon}^{(i)}}{\sqrt{\hat{\sigma}_{(i)}^2 \cdot (1 - h_i)}},
\end{align*}
respectively, where $h_i$ is the $i$th diagonal element of $H$ and $\hat{\sigma}_{(i)}^2 = \frac{1}{n-p-1} \sum_{j \neq i} \hat{\epsilon}^{(j)}$. As \cite[\S~10.2]{Seber:2003} states, $(r^{(i)})^2/(n-p)$ has a $\textrm{beta}[\frac{1}{2}, \frac{1}{2}(n-p-1)]$ distribution which means they are identically distributed (but not independent). The externally Studentized residuals, $t^{(i)}$, have a $t_{n-p-1}$ distribution. The externally Studentized residuals are less prone to outliers than are the internally Studentized residuals.

Consider a graph of $t^{(i)}$ (or $r^{(i)}$) against $\hat{\mu}(x^{(i)})$. If the model is correct, we expect to see a scattering of points with no discernible pattern, since the Studentized residuals would be identically distributed and independent of the mean response. If there is an apparent trend in the residuals, that may indicate a nonlinearity in the model.

To assess a potential dependence between the mean response and the variance, \cite[\S~10.4.2]{Seber:2003} recommends plotting the squared residuals, $\epsilon^{(i)}$, against the fitted means, $\hat{\mu}(x^{(i)})$. If the variance increases with the mean response, this plot will exhibit a wedge shape. We can apply a smoother such as lowess \cite{Cleveland:1979} to estimate the relationship between the mean response and the variance. This gives an estimate of the variance associated with each observation, which can then be used to determine weights for the observations. Since the variance of the $i$th observation is assumed to be $\sigma^2 / w^{(i)}$, and the estimated variance of the $i$th observation is $(\epsilon^{(i)})^2$, we have $w^{(i)} = (\epsilon^{(i)})^{-2}$, where we are setting $\sigma^2=1$ since we are directly estimating the variance of each individual observation.  Iterating on this procedure (estimating the model using weighted least squares, plotting the squared residuals against the mean response, smoothing this plot to estimate the variance of each observation) gives an estimate of $\beta$ that is asymptotically as efficient as knowing the weights a priori.

We can test the normality assumption using a Q-Q plot, which graphs the observed quantiles of the raw residuals against the quantiles of a standard Gaussian distribution. Alternatively we could graph the quantiles of the Studentized residuals against the quantiles of their theoretical distributions \cite[\S~10.5.1]{Seber:2003}.

One of the assumptions of the linear model is that the observations are independent. If the observations have a known correlation structure, various approaches exist for fitting models. If we believe the observations are independent, we can check for a specific deviation from this assumption called \textit{serial correlation}. That is, we can check for correlations between sequential pairs of observations. This is especially relevant when the order of observations is physically meaningful. In the absence of correlation, a residual with positive sign is equally likely to be followed by a residual with positive or negative sign, which can easily be examined graphically. A significance test-based procedure was discussed in a series of papers by Durbin and Watson. This test checks for a first-order autoregressive model for the residuals: $\hat{\epsilon}^{(i)} = \rho \hat{\epsilon}^{(i-1)} + \delta^{(i)}$, where $\delta^{(i)}$ are independent normal variables. Let
\begin{align*}
   D &= \frac{\sum_{i=2}^n (\hat{\epsilon}^{(i)} - \hat{\epsilon}^{(i-1)})^2}{\sum_{i=1}^n (\hat{\epsilon}^{(i)})^2} = \frac{\hat{\epsilon}^T A \hat{\epsilon}}{\hat{\epsilon}^T \hat{\epsilon}} \textrm{, where} \\
   A &= \begin{bmatrix}
       1 & -1 & 0 & 0 & \cdots & \cdots 0 \\
       -1 & 2 & -1 & 0 & \cdots & \cdots & \cdots \\
       0 & -1 & 2 & -1 & \cdots & \cdots & \cdots  \\
       0 & 0 & -1 & 2 & \cdots & \cdots & \cdots \\
       \cdots & \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\
       \cdots & \cdots & \cdots & \cdots & \cdots & 2 & -1 \\
       \cdots & \cdots & \cdots & \cdots & \cdots & -1 & 1
   \end{bmatrix}
\end{align*}
Under the null hypothesis of independent observations, $D$ has the same distribution as
\begin{equation}\label{eqn:SCTS}
    r = \frac{\sum_{i=1}^{n-p} \xi_i \zeta_i^2}{\sum_{i=1}^{n-p} \zeta_i^2},
\end{equation}
where $\zeta_i$ are IID standard Gaussian variables and $\xi_i$ are the nonzero eigenvalues of $(I-H)A$---assuming $V$ is full rank, there will be exactly $n-p$ of these \cite[pg.~416]{DW:50}. Two issues present themselves, one computational, the other theoretical. Computing the eigenvalues of $(I-H)A$ may be computationally intensive if $n-p$ is gigantic or if you happen to be living in 1950. More problematically, exact tail probabilities for distributions of the form~(\ref{eqn:SCTS}) are not available.

Since $H$ depends explicitly on the features, it will be different for each regression analysis; however, it can be shown that the eigenvalues $\xi_i$ of $(I-H)A$ are bounded by pairs of the eigenvalues of $A$, $\lambda_i$ \cite{DW:50}. Specifically, if we sort the eigenvalues so that $\xi_1 \leq \xi_2 \leq \cdots \xi_{n-p}$ and $\lambda_1 \leq \lambda_2 \cdots \leq \lambda_{n}$, then $\lambda_i \leq \xi_i \leq \lambda_{i+p}$, $i=1, 2, \ldots, (n-p)$. The eigenvalues of $A$ have a simple form: $\lambda_j = 2 \cdot ( 1 - \cos( \pi (j-1) / n))$, $j=1, 2, \ldots, n$ \cite[pg.~426]{DW:50}.

Tighter bounds hold whenever $s$ of the eigenvectors of $(I-H)A$ are linear combinations of $s$ of the eigenvectors of $A$. That doesn't seem like it would happen very often, but since $\mathbf{1}$ is an eigenvector of $A$ (corresponding to an eigenvalue of zero), when the model includes a constant affine term, then one of the columns of $V$ is $\mathbf{1}$, and $s \geq 1$. When that happens, we may discard the corresponding $s$ eigenvalues of $A$, leaving $n-s$ eigenvalues $\lambda_i$, and the bounds become $\lambda_i \leq \xi_i \leq \lambda_{i+p-s}$.

These bounds on the eigenvalues of $(I-H)A$ become bounds on the distribution of the Durbin-Watson statistic: $r_L \leq r \leq r_U$, where
\begin{align*}
   r_L &= \frac{\sum_{i=1}^{n-p} \lambda_i \zeta_i^2}{\sum_{i=1}^{n-p} \zeta_i^2}, \\
   r_U &= \frac{\sum_{i=1}^{n-p} \lambda_{i+p-s} \zeta_i^2}{\sum_{i=1}^{n-p} \zeta_i^2}.
\end{align*}
Note that the distributions of $r_L$ and $r_U$ depend on $n$, $p$, and $s$, but not on the features.

It is straightforward to show that $r_L \geq \lambda_1 = 0$ and $r_U \leq \lambda_n < 4$, which shows that the Durbin-Watson statistic satisfies $0 \leq D < 4$. In the presence of positive serial correlation, $\rho > 0$, $D$ will tend to be closer to 0. When $\rho < 0$, $D$ will tend to be closer to 4. In the absence of serial correlation, $D$ will typically be close to 2. Let $\Phi_L$, $\Phi_U$, and $\Phi_{\textrm{DW}}$ be the cumulative distribution functions of $r_L$, $r_U$, and $r$, respectively, so that, for example, $\Phi_L^{-1}(d) = \Prob\left\{ r_L \leq d \right\}$. In light of the above discussion, $\Phi_L^{-1}(d) \geq \Phi_{\textrm{DW}}^{-1}(d) \geq \Phi_U^{-1}(d)$ \cite[pg.~418]{DW:50}. Table~\ref{tbl:DW} shows how these functions provide bounds on the p-values for various tests related to serial correlation. Evaluating the exact p-values require the eigenvalues $\xi_i$, which depend on the features. Evaluating the bounds only requires tail-probabilities for $\Phi_L$ and $\Phi_U$, which do not depend on the features (but do depend on $n$, $p$, and $s$). These have been tabulated for various values of $n$, $p$, and $s$, for example in \cite{DW:51}.

\begin{table}[!b]
\centering
\begin{tabular}{cccc}
   Test & p-value & Lower bound & Upper bound \\
   $\rho = 0$ vs.~$\rho > 0$ & $\Phi_{\textrm{DW}}^{-1}(d)$ & $\Phi_U^{-1}(d)$ & $\Phi_L^{-1}(d)$ \\
   $\rho = 0$ vs.~$\rho < 0$ & $1-\Phi_{\textrm{DW}}^{-1}(d)$ & $1-\Phi_L^{-1}(d)$ & $1-\Phi_U^{-1}(d)$ \\
   $\rho = 0$ vs.~$\rho \neq 0$ & $\Phi_{\textrm{DW}}^{-1}(d^\prime) + 1 - \Phi_{\textrm{DW}}^{-1}(d^\prime)$ & $\Phi_U^{-1}(d^\prime) + 1 - \Phi_L^{-1}(d^\prime)$ & $\Phi_L^{-1}(d^\prime) + 1 - \Phi_U^{-1}(d^\prime)$
\end{tabular}
\caption{\label{tbl:DW}Bounds on p-values for one and two-sided tests regarding the correlation parameter, $\rho$. In all cases, $d$ is the observed value of the Durbin-Watson statistic. In the last row, $d^\prime = 2 - |2 - d|$.}
\end{table}

When performing an analysis with numbers of observations and parameters not represented in an available table, we are still left with the challenge of computing the tail probabilities of $r_L$ and $r_U$. We may proceed by approximating $r_L / 4$ and $r_U / 4$ as beta-distributed. Tail probabilities for the beta distribution may then be mapped to p-values for the Durbin-Watson statistic. The beta distributions are chosen to have the same means and variances as $r_L/4$ and $r_U/4$, respectively. These may be expressed in terms of the eigenvalues of $A$, $\lambda_i$. If the eigenvalues, $\xi_i$, are in fact known, we can dispense with the bounds entirely. Means and variances for all three statistics are reported in Table~\ref{tbl:DWMV}.

\begin{table}
\centering
\begin{tabular}{ccc}
Statistic & Mean & Variance \\
$r$ & $\mu=\frac{1}{n-p} \sum_{i=1}^{n-p} \xi_i$ & $\sigma^2 = \frac{2 \sum_{i=1}^{n-p} (\xi_i - \mu)^2}{(n-p)(n-p+2)}$ \\
$r_L$ & $\mu_L=\frac{1}{n-p} \sum_{i=1}^{n-p} \lambda_i$ & $\sigma_L^2 = \frac{2 \sum_{i=1}^{n-p} (\lambda_i - \mu_L)^2}{(n-p)(n-p+2)}$ \\
$r_U$ & $\mu_U=\frac{1}{n-p} \sum_{i=1}^{n-p} \lambda_{i+p-s}$ & $\sigma_U^2 = \frac{2 \sum_{i=1}^{n-p} (\lambda_{i+p-s} - \mu_U)^2}{(n-p)(n-p+2)}$
\end{tabular}
\caption{\label{tbl:DWMV} Means and variances of the Durbin-Watson and related statistics}
\end{table}

A beta distribution is typically characterized by parameters $\alpha$ and $\beta$. The mean of a beta distribution is $\frac{\alpha}{\alpha + \beta}$ and the variance is $\frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}$, so the beta distribution used to approximate $r$, for example, has parameters
\begin{align*}
   \alpha &= \frac{\mu^2 ( 4 - \mu)}{4 \sigma^2} - \frac{\mu}{4}, \\
   \beta &= \frac{\mu ( 4 - \mu)^2}{4 \sigma^2} - \frac{4 - \mu}{4}.
\end{align*}
For the purposes of calculating p-values, $\Phi_{\alpha, \beta}^{-1}(d/4)$ may be substituted for $\Phi_{\textrm{DW}}^{-1}(d)$ in Table~\ref{tbl:DW}, where $\Phi_{\alpha, \beta}$ is the cumulative distribution function of a beta random variable with parameters $\alpha$ and $\beta$. For more details and discussion of alternative approaches, see \cite{DW:71}.

\subsection{Isolated Departures from the Model}
Finally, we want to examine how any potential outliers affect the fitted model. Small changes to the response for observations on the outskirts of the feature space can have a big effect on the model; such points are said to have high \textit{leverage}. Leverage may be investigated by examining the diagonal terms of the hat matrix, $h_i$. The higher a particular $h_i$, the larger the influence of the corresponding observation on the fitted model. We might consider any observation having $h_i > 2 p / n$ to have high leverage \cite[10.6.1]{Seber:2003}. An observation that does not have high leverage, but deviates wildly from the mean response can also have undue influence on the model. Since the externally Studentized residuals have a $t_{n-p-1}$ distribution, any residual with $|t^{(i)}| > 2$ should be examined (corresponding approximately to the upper and lower $2.5\%$ quantiles).

If a particular observation is both an outlier and has high leverage, we can try omitting the observation, or reducing its weight, and refitting. Large changes to the fitted model indicate the point has high influence. Deciding whether or not to remove the observation depends on the goals of the analysis, how the data were collected, and so forth. A variety of statistics are available for quantifying the impact of leaving out a single observation without actually having to refit the model\cite[\S~10.6.3]{Seber:2003}. For example, the impact of leaving out the $i$th observation to the $i$th fitted value is $h_i \epsilon^{(i)} / (1 - h_i)$. This can be standardized giving $t^{(i)} \sqrt{h_i / (1 - h_i)}$. A cutoff of $2\sqrt{p / (n-p)}$ can be used for identifying high influence points. Another statistic, called Cook's $D$, may be written:
\begin{displaymath}
   D^{(i)} = \left( r^{(i)} \right)^2 \frac{h_i}{p(1-h_i)}.
\end{displaymath}
Cook recommended using $F_{p, n-p}^{0.10}$ as the cutoff for identifying high influence points \cite{Cook:1977}.

When one or more observations have been identified as possible outliers, a formal test may be applied \cite[\S~10.6.4]{Seber:2003}. If we are testing a set of $k$ observations, we augment $\nu$ with $k$ extra entries. The $j$th of these entries is 1 for the $j$th potential outlier, and zero otherwise. We fit the expanded model. Let $\hat{\gamma}$ be the subset of entries of $\hat{\beta}$ corresponding to these extra entries of $\nu$. If the unaugmented model is correct, and the observations under consideration are \textit{not} outliers, then $\gamma = 0$. The $F$-test outlined in \S~\ref{sec:CIHT} may be applied to test this hypothesis, giving a p-value for the collection of potential outliers. In this case, the test statistic is:
\begin{displaymath}
   T_C = \frac{1}{k} \| \hat{\gamma} \|_{\hat{\mathcal{I}}_{\textrm{augmented}}^{-1}}^2 \sim F_{k, n-p-k},
\end{displaymath}
where $\hat{\mathcal{I}}_{\textrm{augmented}}^{-1}$ is the $k \times k$ submatrix of $\hat{\mathcal{I}}^{-1}$ corresponding to the augmented entries of $\nu$. The p-value is $1 - \Phi_{F_{k, n-p-k}}(T_C)$. If this p-value is small, that constitutes evidence that at least one of the observations under consideration is an outlier.

\section{Properties of Generalized Linear Models}
\label{app:glm}

In this section, we state various useful facts (without proof) regarding generalized linear models. For details, see \cite{MN:1983}, \cite{Wood:2017}, and \cite{Agresti:2012}. Generalized linear models have large sample properties similar to linear models, but with finite data these relationships are not necessarily practical. The primary goals of this section are to show how to map generalized linear models to the results of Appendix \ref{app:linear_model} and to point out where other methods might be more reliable.

\subsection{Properties of the Estimated Model}

Suppose the distribution of $Y \given X = x$ is in an exponential family; specifically the distribution is in the \emph{same} exponential family for all values of $x$. Examples include the normal, binomial, Poisson, and gamma distributions. These distributions have a specific relationship between the mean and variance: if $\Exp [Y \given X = x] = \mu(x)$ and $\Var(Y \given X = x) = \sigma^2 (x)$, then distributions in the exponential family all satisfy $\sigma^2(x) = U(\mu(x)) \cdot a(\phi)$; that is, the variance depends on $x$ only through the mean $\mu$ as well as on a parameter $\phi$, called the dispersion, which does not depend on $x$ \cite[\S 2.2.2]{MN:1983}. (In the case of the binomial and Poisson distributions, the dispersion is automatically equal to one; these are one-parameter exponential families.) $U(\mu)$ is called the \emph{variance function}. The variance function is distinct from the variance!

The important property needed for generalized linear models is the relationship between the mean and variance, which means we can expand generalized linear models to encompass any distribution where this relationship is known \cite[\S 9]{MN:1983}. For example, if we believe the variance increases linearly with the mean, we can use the methods described herein, without actually knowing the details of the distribution. We see that generalized linear models are indeed quite general!

Now suppose that $g(\mu(x)) = \eta(x) = \nu(x)^T \beta$, where $g$ (called the \emph{link function}) is a known monotonic differentiable function, and $\nu(x) \in \reals^p$ is a known function. A generalized linear model is characterized by the distribution of $Y \given X = x$ (or the more generally the relationship between the mean and variance of this distribution), the link function $g$ and the design function $\nu$. For example, when $g$ is the identity, and the normal distribution is chosen, we recover the linear model.

We fit generalized linear models using Maximum Likelihood Estimation (MLE), or an alternative method based on a `quasi-likelihood function' when the exact distribution is unknown. Let $\ell(\mu(\beta); x, y) = \sum_i \ell(\mu(\beta); x^{(i)}, y^{(i)})$ be the log-likelihood or log-quasi-likelihood expressed as a function of the parameters $\beta$ and parameterized by the data. Whichever value of $\beta$ maximizes the log-likelihood (and therefore the likelihood) is the maximum likelihood estimate of $\beta$. For many choices of distribution and link function, maximum likelihood estimation corresponds to a convex optimization problem and can be efficiently performed. We will restrict our attention to such combinations.

The log-likelihood function achieves its maximum possible value when $\mu = y$; that is, $\ell(y; x, y)$ is the largest possible log-likelihood. Often there is no choice of $\beta$ such that $\mu(\beta) = y$, due to restrictions imposed by the design function, $\nu$. The scaled deviance, $D^*$, for a fitted model with parameter $\hat{\beta}$ is defined to be $D^* = 2 \ell(y; x, y) - 2 \ell(\mu(\hat{\beta}); x, y)$. Necessarily, $D^* \geq 0$, with smaller values of $D^*$ indicating a better fit. The scaled deviance has a certain relationship with the $\chi^2$ distribution which is frequently used for inference. Unfortunately, we need to know the dispersion parameter, $\phi$, in order to compute the scaled deviance. Often it is better to work with the (unscaled) deviance, $D = D^* \cdot \phi$ which does not involve the dispersion parameter.

Let $\{ (x^{(i)}, y^{(i)}) \}_{i=1, \ldots, n}$ be IID samples drawn from the joint distribution of $X$ and $Y$. Let $V$ be the matrix whose $i$th row is $\nu(x^{(i)})$ and assume $V$ is full rank. Then the Maximum Likelihood Estimate of $\beta$, $\hat{\beta}$ satisfies $V^T W V \hat{\beta} = V^T W z$, where $W^{-1} = \diag(g^\prime(\hat{\mu}^{(i)})^2 \cdot U(\hat{\mu}^{(i)}))$ and $z_i = \hat{\eta}^{(i)} + (y^{(i)} - \hat{\mu}^{(i)}) g^\prime(\hat{\mu}^{(i)})$. In turn, $\hat{\eta}^{(i)} = \nu(x^{(i)})^T \hat{\beta}$ and $g(\hat{\mu}^{(i)}) = \hat{\eta}^{(i)}$. These equations can be used to determine $\hat{\beta}$ using an iterative weighted least squares procedure \cite[\S 2.5]{MN:1983}, but that is not the procedure \gamdist{} uses. Just as importantly, this fact can be used to derive the large sample properties of $\hat{\beta}$ and related quantities.

For example, the large sample distribution of $\hat{\beta}$ is $\normal(\beta,  \mathcal{I}^{-1})$, where $\mathcal{I} = V^T W V / \phi$ is the Fisher information, which shows that $\hat{\beta}$ is asymptotically unbiased. (It is also a consistent estimator, meaning that under certain regularity conditions, as $n \to \infty$, $\hat{\beta}$ converges in probability $\beta$.) Many of the formulae of Appendix~\ref{app:linear_model} hold asymptotically with these equations for $\hat{\beta}$, $\mathcal{I}$, and the estimated Fisher information $\hat{\mathcal{I}} = V^T W V / \hat{\phi}$, where $\hat{\phi}$ is an estimate of the dispersion (see below). These formulae can be used for hypothesis tests on linear combinations of the parameters $\beta$, to compute the power of these tests against specific alternatives, and to compute confidence intervals on these linear combinations.

\subsection{Methods with Better Finite-Sample Performance}
In a few instances, methods with better performance with finite data are known. These methods outperform the asymptotic-normal approach for moderate data sizes.  For example, the likelihood ratio test often provides better confidence intervals on $\beta$ than methods based on the normal approximation. Any value of $\beta$ satisfying 
\begin{displaymath}
   2 \ell(\hat{\beta}; x, y) - 2 \ell(\beta; x, y) \leq \Phi_{\chi_p^2}(1 - \alpha)
\end{displaymath}
is part of a $100(1-\alpha)\%$ confidence region on the true parameter.

Now suppose $H_0$ and $H_1$ are nested models with $p-k$ and $p$ degrees of freedom, respectively, $k > 0$. By nested we mean that $H_0$ is simply the model $H_1$ subject to a constraint $C \beta = d$, where $C \in \reals^{k\times p}$. Let $D_i^*$ be the scaled deviance of the model fit under $H_i$. Then under $H_0$,
\begin{displaymath}
   T_{\textrm{LR}}(d) = D_0^* - D_1^*
\end{displaymath}
has an asymptotic $\chi_{k}^2$ distribution. Computing the scaled deviance requires knowledge of the dispersion parameter. When this is unknown, rather than substituting an estimate it is better to work in terms of the deviances, $D_i = D_i^* \cdot \phi$, which do not require knowledge of $\phi$ to compute. Then under $H_0$,
\begin{displaymath}
   T_F(d) = \frac{(D_0 - D_1) / k}{D_1 / (n - p)}
\end{displaymath}
has an asymptotic $F_{k, n-p}$ distribution. These facts can be used to compute p-values on the null hypothesis $H_0$ against the alternative $H_1$. We will refer to these tests as the likelihood ratio test and generalized $F$-test, respectively. These tests are preferred (over their counterparts described in Appendix~\ref{app:linear_model}) for model selection with generalized linear models. Other methods described in Appendix~\ref{app:linear_model} such as the Akaike Information Criterion (the general formula is $\textrm{AIC} = -2 \cdot \ell + 2 \cdot \textrm{dof}$) and cross validation are valid here as well \cite[\S 3.1.4]{Wood:2017}.

Computing the power of these tests is challenging. One approach is simulation based. Suppose we want to calculate the power of the likelihood ratio test or  the generalized $F$-test against an alternative, $C \beta = d^\prime$. We must translate that into a specific $\beta$ for the purposes of computing the power. Often this is accomplishable using historic data we believe to be representative of the test at hand. Given a set of historic data $\{ (x_h^{(i)}, y_h^{(i)}) \}_{i=1, \cdots, n_h}$, we simply fit a model to these historic data under the constraint that $C \beta = d^\prime$. The resulting $\hat{\beta}$ is consistent with both the alternative hypothesis and historic data. We might even compute the power at various $\beta$ perhaps drawn from a confidence region of that historic fit, which gives a more robust assessment of the power of the test we wish to conduct. Alternatively, we can specify a plausible collection of values of $\mu$ corresponding to different sets of features and use that to determine $\beta$ consistent with the alternative hypothesis.

However we arrive at an alternative $\beta$, we can simply calculate $\mu^{(i)} = g^{-1}(\nu(x^{(i)})^T \beta)$ for any desired $x^{(i)}$. From $\mu^{(i)}$ we can sample $y^{(i)}$ from the conditional distribution. For example, when planning an experiment, we would decide, for any given sample size, how to assign features to experimental units. Thus we can calculate $x^{(i)}$ in advance, and sample $y^{(i)}$ from the distribution corresponding to the alternative hypothesis. Then we compute the test statistic, $T_{\textrm{LR}}(d)$ or $T_F(d)$, which requires fitting the model under $H_1$ and $H_0$ to the simulated data. If the resulting statistic is greater than the corresponding tail value, we would reject the null hypothesis, indicating we were successfully able to distinguish the alternative from the null. We would repeat this procedure many times, resampling $y^{(i)}$  each time. The fraction of simulations in which we reject the null is a consistent estimate of the power of the test against the alternative \cite{JBFM:15}. 

Suppose we desire $80\%$ power. A few hundred simulations should be adequate to ensure the power of the test is approximately $80\%$. These calculations could be done in parallel to leverage multiple processors. In particularly sensitive applications, we might need to do thousands of simulations, which could be computationally expensive. An alternative approach is based on approximating the test statistics as having noncentral $\chi^2$ or $F$ distributions under the alternative hypothesis. 

We begin by translating the constraint, $C\beta = d$ into new coordinates. We have assumed that $C \in \reals^{k \times p}$, $k < p$, is full rank, so $C$ may be written as the product of a lower triangular matrix, $L \in \reals^{k \times p}$ and a unitary matrix $Q \in \reals^{p \times p}$. This decomposition may be achieved by computing the QR decomposition of $C^T$ and simply transposing the resulting matrices. We can partition $L$ into $\left[ \begin{array}{ll} L_1 & 0 \end{array}\right]$ and $Q$ into $\left[ \begin{array}{l} Q_1 \\ Q_2 \end{array} \right]$, where $L_1 \in \reals^{k \times k}$ is lower triangular, $Q_1 \in \reals^{k \times p}$ has orthonormal rows, and $Q_2 \in \reals^{(p-k) \times p}$ does too. Then $C = LQ = L_1 Q_1$. Define $\psi = Q_1 \beta$ and $\lambda = Q_2 \beta$ so that $\left[ \begin{array}{ll} \psi & \lambda \end{array}\right]^T = Q \beta$. Since $C = LQ = L_1 Q_1$, $C \beta = L_1 Q_1 \beta = L_1 \psi$. Thus the constraint $C \beta = d$ may be translated into the equivalent $L_1 \psi = d$ or $\psi = L_1^{-1} d =: \psi_0$. Recalling that $\eta(x^{(i)}) = \nu(x^{(i)})^T \beta$, we redefine this as $\eta(x^{(i)}) = Z_i^T \psi + X_i^T \lambda$, where $Z_i = Q_1 \nu(x^{(i)})$ and $X_i = Q_2 \nu(x^{(i)})$. We may then rewrite the null hypothesis as $H_0: \psi = \psi_0$ versus the alternative $H_1: \psi \neq \psi_0$. We wish to compute the power against the alternative $C \beta = d^\prime$ which is translated into $\psi = L_1^{-1} d^\prime =: \psi^\prime$. To do so, we also need a value for $\lambda$ associated with the alternative hypothesis. We will call this value $\lambda^\prime$.

The basic ingredients of the power calculation for the likelihood ratio test are the null hypothesis for $\psi$, $\psi_0$; the alternative hypotheses for $\psi$ and $\lambda$, $\psi^\prime$ and $\lambda^\prime$ respectively; and the expected MLE of $\lambda$ under the null hypothesis, in the limit of infinite data, which we will refer to as $\lambda_0^*$. The last ingredient is perhaps the only challenging one. If we can generate features corresponding to a large number of observations (e.g.~millions of data points), we can compute the mean response under the alternative hypothesis and fit the model to the features and mean response under the null hypothesis. The resulting $\lambda$ should be a reasonable approximation to $\lambda_0^*$. For an alternative approach see \cite{SM:88}.

The noncentrality parameter associated with the likelihood ratio test is
\begin{displaymath}
   \gamma = k - \Xi + 2 \sum_{i=1}^n a_i^{-1}(\phi) \left[ b^\prime (\theta_i) \left( \theta_i - \theta_i^* \right) - \left( b(\theta_i) - b(\theta_i^*) \right) \right],
\end{displaymath}
where $\Xi$ is as described below, $\theta_i$ is the canonical parameter for the $i$th observation evaluated at $\psi^\prime$ and $\lambda^\prime$, and $\theta_i^*$ is the same but evaluated at $\psi_0$ and $\lambda_0^*$. The term $\Xi$ is the trace of a matrix as described in \cite{SMO:92}, but \cite{Shieh:00} indicates $\Xi \approx k$ which simplifies the calculation of the statistic considerably. The power of the likelihood ratio test is simply $1 - \Phi_{\chi_{k;\gamma}^2}\left( \Phi_{\chi_k^2}^{-1} ( 1 - \alpha ) \right)$.

To compute the power associated with the generalized $F$ test, write the test statistic as
\begin{displaymath}
   T_F(d) = \frac{(D_0^* - D_1^*) / k}{D_1^* / (n-p)}.
\end{displaymath}
Under the alternative hypothesis, the numerator is a $\chi_{k, \gamma}^2$ random variable divided by its degrees of freedom, and the denominator is a $\chi_{n-p}^2$ random variable divided by its degrees of freedom, which shows that $T_F$ has a noncentral $F$ distribution. The power of this test is $1 - \Phi_{F_{k, n-p; \gamma}}(F_{k, n-p}^{1 - \alpha})$.

As in Appendix~\ref{app:linear_model}, we may use the likelihood ratio test or the generalized $F$ test to derive an approximate confidence region on $C\beta$. For example, an asymptotic, approximate $100(1-\alpha)\%$ confidence region on $C\beta$ is $\{ d \, : \, T_F(d) \leq F_{k, n-p}^{1 - \alpha} \}$. Practically, for each candidate $d$ we have to refit $H_0$ to evaluate $T_F(d)$. It is not obvious how best to characterize this region, but I have some thoughts. First, I anticipate this region is convex, though I have not yet attempted to prove this. If $\hat{\beta}_1$ is the MLE of $\beta$ under $H_1$, then $T_F(C \hat{\beta}_1) = 0$ and thus $C \hat{\beta}_1$ is in the confidence region for any $\alpha$. We can compute the boundaries in particular directions $u$ by finding the minimum and maximum values of $\xi$ such that $T_F(C \hat{\beta}_1 + \xi u) \leq F_{k, n-p}^{1 - \alpha}$. Then, we can approximate the region by the smallest ellipsoid that encompasses the discovered boundary points. This is a convex optimization problem and can be solved quickly. In light of the confidence region for the $F$-test for linear models, I anticipate the confidence region is approximately, perhaps asymptotically, ellipsoid.

\subsection{Estimating the Dispersion Parameter}
There are several ways of estimating the dispersion parameter based on the distribution. One method, called the Pearson estimator, is based on Pearson's $X^2$ statistic:
\begin{displaymath}
   \hat{\phi}_P = \frac{1}{n-p} \sum_i \frac{(y^{(i)} - \hat{\mu}^{(i)})^2}{U(\mu^{(i)})},
\end{displaymath}
where $U(\mu)$ is the variance function (recall that the variance function is distinct from the variance). The Pearson estimator is asymptotically unbiased for the dispersion, as its asymptotic distribution shows: $(n-p)\hat{\phi}_P / \phi \sim \chi_{n-p}^2$. This relationship can be used to compute confidence intervals on the dispersion, similar to Equation~(\ref{eqn:var_ci}).

For particular distributions, in certain circumstances, more effective methods for estimating the dispersion are possible. See, for example, \cite[\S 4.5.2, \S 6.2.4, and \S 8.3.6]{MN:1983}. One promising replacement is Fletcher's estimator, introduced in \cite{Fletcher:12} and described in \cite[\S 3.1.5]{Wood:2017}: $\hat{\phi}_F = \frac{\hat{\phi}_P}{1 + \bar{s}}$ where $\bar{s}=n^{-1} \sum_{i=1}^n U^\prime(\hat{\mu}^{(i)}) (y^{(i)} - \hat{\mu}^{(i)})/U(\hat{\mu}^{(i)})$.

\begin{itemize}
\item What is the distribution of $T_F(d)$ under an alternative hypothesis, $C \beta = d^\prime$? I think it's a noncentral $F$ with noncentrality parameter relating to the difference in likelihoods between $H_0$ and the specific alternative. $D_0 - D_0^\prime$ where $D_0^\prime$ is the deviance under the alternative. What is the power of this test?
\item Prediction intervals? Methods above yield standard errors on $\hat{\mu}(x_\textrm{new})$. Add variance of this to variance of distribution and take square root to get standard errors on predictions.
\item How to check model assumptions? Graph-based and formal? What are the residuals and what do we do with them? How can we check for serial correlation? How can we check for deviations from linearity in $\eta$? How can we check the link function? How can we check the variance function? How can we check the distribution? How do we detect outliers? Leverage and influence? Formal test for outliers (should be similar to $F$-test?)
\item What is the score test here?
\item What is the p-value for the lrt?
\end{itemize}

\section{Models with Regularization}
\label{app:regularization}

\begin{itemize}
\item Bayesian interpretation
\item What is the impact to hypothesis tests? Confidence intervals? Power? Predictions?
\item Model selection?
\item Analysis of residuals? Outliers, leverage, influence?
\end{itemize}

\newpage
\bibliography{gamdist}

\end{document}
